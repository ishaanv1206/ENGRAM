# Main Language Model Configuration (for response generation)
MAIN_LLM_MODEL_PATH=./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
MAIN_LLM_N_CTX=4096
MAIN_LLM_N_GPU_LAYERS=99

# Small Language Model Configuration (for memory analysis)
# Using Dolphin 8B model
SLM_MODEL_PATH=./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
# SLM_LORA_PATH=./data/models/fine_tuned_lora.gguf 
SLM_N_CTX=1024
SLM_N_GPU_LAYERS=99

# Embedding Model Configuration (for semantic search)
EMBEDDING_MODEL_PATH=./data/models/mxbai-embed-large-v1_fp32.gguf
EMBEDDING_N_CTX=512
EMBEDDING_N_GPU_LAYERS=-1

# Storage Paths
PINNED_MEMORY_PATH=./data/pinned_memory.json
SESSION_STATE_PATH=./data/sessions/
ARCHIVE_PATH=./data/archive/

# System Configuration
LOG_LEVEL=INFO
MAX_CONVERSATION_TURNS=10
MEMORY_BUDGET_TOKENS=500
CACHE_SIZE_TIER1=50
