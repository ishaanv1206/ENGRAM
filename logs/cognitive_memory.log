2026-02-07 20:03:01 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 20:03:01 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 20:03:01 - __main__ - INFO - Storage directory ready: data
2026-02-07 20:03:01 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 20:03:01 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 20:03:01 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 20:03:01 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 20:03:01 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 20:03:02 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 20:03:02 - __main__ - ERROR - Failed to initialize pipeline: GraphMemoryEngine.__init__() got an unexpected keyword argument 'uri'
Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 132, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 78, in __init__
    self.graph_engine = GraphMemoryEngine(
                        ^^^^^^^^^^^^^^^^^^
TypeError: GraphMemoryEngine.__init__() got an unexpected keyword argument 'uri'
2026-02-07 20:03:02 - __main__ - ERROR - Unexpected error in main loop: GraphMemoryEngine.__init__() got an unexpected keyword argument 'uri'
Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 243, in async_main
    pipeline = await initialize_pipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\main.py", line 132, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 78, in __init__
    self.graph_engine = GraphMemoryEngine(
                        ^^^^^^^^^^^^^^^^^^
TypeError: GraphMemoryEngine.__init__() got an unexpected keyword argument 'uri'
2026-02-07 20:03:02 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 20:06:32 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 20:06:32 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 20:06:32 - __main__ - INFO - Storage directory ready: data
2026-02-07 20:06:32 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 20:06:32 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 20:06:32 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 20:06:32 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 20:06:32 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 20:06:33 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 20:06:34 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 20:06:35 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 20:06:35 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 20:06:35 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 20:06:36 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 20:06:36 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 20:06:36 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 20:06:36 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 20:06:36 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:06:36 - src.pipeline - INFO - Starting background tasks...
2026-02-07 20:06:36 - src.pipeline - INFO - Reflection loop started
2026-02-07 20:06:36 - src.pipeline - INFO - Decay manager started
2026-02-07 20:06:36 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 20:06:36 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:06:36 - __main__ - INFO - Creating Gradio web interface...
2026-02-07 20:06:36 - src.gradio_ui - INFO - New conversation started with session_id: 164f4797-f194-404e-8558-48cbfa696c45
2026-02-07 20:06:36 - src.gradio_ui - INFO - GradioUI initialized
2026-02-07 20:06:36 - __main__ - INFO - Gradio interface created successfully
2026-02-07 20:06:36 - src.gradio_ui - INFO - Creating Gradio interface...
2026-02-07 20:06:37 - src.gradio_ui - INFO - Launching Gradio interface on 127.0.0.1:7860
2026-02-07 20:11:35 - src.gradio_ui - INFO - Processing user message: Hi
...
2026-02-07 20:11:35 - src.pipeline - INFO - Processing turn 1: Hi...
2026-02-07 20:12:19 - src.memory_analyzer - WARNING - No JSON found in SLM response, using fallback
2026-02-07 20:12:19 - src.memory_analyzer - INFO - Creating fallback extraction for failed analysis
2026-02-07 20:12:31 - src.retrieval_gatekeeper - INFO - Retrieval completed in 11623.46ms: 0 memories, 0 tokens
2026-02-07 20:12:59 - src.pipeline - INFO - Response generated: How can I assist you today?...
2026-02-07 20:12:59 - src.gradio_ui - ERROR - Error processing message: 'CognitivePipeline' object has no attribute '_persist_session'
Traceback (most recent call last):
  File "D:\Neurohacks\src\gradio_ui.py", line 73, in handle_text
    response = await self.pipeline.process_turn(message.strip(), self.context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 251, in process_turn
    asyncio.create_task(self._persist_session(context))
                        ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'CognitivePipeline' object has no attribute '_persist_session'
2026-02-07 20:13:22 - src.gradio_ui - INFO - Processing user message: hi...
2026-02-07 20:13:22 - src.pipeline - INFO - Processing turn 2: hi...
2026-02-07 20:13:57 - src.memory_analyzer - WARNING - No JSON found in SLM response, using fallback
2026-02-07 20:13:57 - src.memory_analyzer - INFO - Creating fallback extraction for failed analysis
2026-02-07 20:14:08 - src.retrieval_gatekeeper - INFO - Retrieval completed in 11558.61ms: 0 memories, 0 tokens
2026-02-07 20:14:27 - src.pipeline - INFO - Response generated: Is there something on your mind that you'd like to...
2026-02-07 20:14:27 - src.gradio_ui - ERROR - Error processing message: 'CognitivePipeline' object has no attribute '_persist_session'
Traceback (most recent call last):
  File "D:\Neurohacks\src\gradio_ui.py", line 73, in handle_text
    response = await self.pipeline.process_turn(message.strip(), self.context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 251, in process_turn
    asyncio.create_task(self._persist_session(context))
                        ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'CognitivePipeline' object has no attribute '_persist_session'
2026-02-07 20:19:33 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 20:19:33 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 20:19:33 - __main__ - INFO - Storage directory ready: data
2026-02-07 20:19:33 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 20:19:33 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 20:19:33 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 20:19:33 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 20:19:33 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 20:19:33 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 20:19:34 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 20:19:35 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 20:19:35 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 20:19:35 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 20:19:36 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 20:19:36 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 20:19:36 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 20:19:36 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 20:19:36 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:19:36 - src.pipeline - INFO - Starting background tasks...
2026-02-07 20:19:36 - src.pipeline - INFO - Reflection loop started
2026-02-07 20:19:36 - src.pipeline - INFO - Decay manager started
2026-02-07 20:19:36 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 20:19:36 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:19:36 - __main__ - INFO - Creating Gradio web interface...
2026-02-07 20:19:36 - src.gradio_ui - INFO - New conversation started with session_id: e31387f5-fa26-46cf-b7b8-9f0accf0015b
2026-02-07 20:19:36 - src.gradio_ui - INFO - GradioUI initialized
2026-02-07 20:19:36 - __main__ - INFO - Gradio interface created successfully
2026-02-07 20:19:36 - src.gradio_ui - INFO - Creating Gradio interface...
2026-02-07 20:19:36 - src.gradio_ui - INFO - Launching Gradio interface on 127.0.0.1:7860
2026-02-07 20:19:36 - __main__ - ERROR - Unexpected error in main loop: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.
Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 262, in async_main
    ui.launch(
  File "D:\Neurohacks\src\gradio_ui.py", line 268, in launch
    interface.launch(
  File "D:\Neurohacks\venv\Lib\site-packages\gradio\blocks.py", line 2733, in launch
    ) = http_server.start_server(
        ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\gradio\http_server.py", line 182, in start_server
    raise OSError(
OSError: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.
2026-02-07 20:19:36 - src.pipeline - INFO - Stopping background tasks...
2026-02-07 20:19:36 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-07 20:19:36 - src.decay_manager - INFO - Decay Manager stopped
2026-02-07 20:19:36 - src.pipeline - INFO - All background tasks stopped
2026-02-07 20:19:38 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 20:19:38 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 20:19:38 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 20:24:40 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 20:24:40 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 20:24:40 - __main__ - INFO - Storage directory ready: data
2026-02-07 20:24:40 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 20:24:40 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 20:24:40 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 20:24:40 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 20:24:40 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 20:24:41 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 20:24:42 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 20:24:43 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 20:24:43 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 20:24:43 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 20:24:44 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 20:24:44 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 20:24:44 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 20:24:44 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 20:24:44 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:24:44 - src.pipeline - INFO - Starting background tasks...
2026-02-07 20:24:44 - src.pipeline - INFO - Reflection loop started
2026-02-07 20:24:44 - src.pipeline - INFO - Decay manager started
2026-02-07 20:24:44 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 20:24:44 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:24:44 - __main__ - INFO - Creating Gradio web interface...
2026-02-07 20:24:44 - src.gradio_ui - INFO - New conversation started with session_id: 9a413aa5-e802-46cc-a8b6-89c7a27e26fe
2026-02-07 20:24:44 - src.gradio_ui - INFO - GradioUI initialized
2026-02-07 20:24:44 - __main__ - INFO - Gradio interface created successfully
2026-02-07 20:24:44 - src.gradio_ui - INFO - Creating Gradio interface...
2026-02-07 20:24:44 - src.gradio_ui - INFO - Launching Gradio interface on 127.0.0.1:7861
2026-02-07 20:24:58 - src.gradio_ui - INFO - Processing user message: hi
...
2026-02-07 20:24:58 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-07 20:25:48 - src.memory_analyzer - WARNING - No JSON found in SLM response, using fallback
2026-02-07 20:25:48 - src.memory_analyzer - INFO - Creating fallback extraction for failed analysis
2026-02-07 20:25:59 - src.retrieval_gatekeeper - INFO - Retrieval completed in 10893.34ms: 0 memories, 0 tokens
2026-02-07 20:26:16 - src.pipeline - INFO - Response generated: How can I assist you today?...
2026-02-07 20:26:16 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 20:26:16 - src.gradio_ui - INFO - Response generated and added to history
2026-02-07 20:35:09 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 20:35:09 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 20:35:09 - __main__ - INFO - Storage directory ready: data
2026-02-07 20:35:09 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 20:35:09 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 20:35:09 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 20:35:09 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 20:35:09 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 20:35:10 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 20:35:11 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 20:35:12 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 20:35:12 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 20:35:12 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 20:35:14 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 20:35:14 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 20:35:14 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 20:35:14 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 20:35:14 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:35:14 - src.pipeline - INFO - Starting background tasks...
2026-02-07 20:35:14 - src.pipeline - INFO - Reflection loop started
2026-02-07 20:35:14 - src.pipeline - INFO - Decay manager started
2026-02-07 20:35:14 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 20:35:14 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:35:14 - __main__ - INFO - Creating Gradio web interface...
2026-02-07 20:35:14 - src.gradio_ui - INFO - New conversation started with session_id: 407ff45c-29a9-41ea-ba45-5e5a23cc6558
2026-02-07 20:35:14 - src.gradio_ui - INFO - GradioUI initialized
2026-02-07 20:35:14 - __main__ - INFO - Gradio interface created successfully
2026-02-07 20:35:14 - src.gradio_ui - INFO - Creating Gradio interface...
2026-02-07 20:35:14 - __main__ - ERROR - Unexpected error in main loop: Chatbot.__init__() got an unexpected keyword argument 'type'
Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 262, in async_main
    ui.launch(
  File "D:\Neurohacks\src\gradio_ui.py", line 167, in launch
    chatbot = gr.Chatbot(
              ^^^^^^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\gradio\component_meta.py", line 194, in wrapper
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Chatbot.__init__() got an unexpected keyword argument 'type'
2026-02-07 20:35:14 - src.pipeline - INFO - Stopping background tasks...
2026-02-07 20:35:14 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-07 20:35:14 - src.decay_manager - INFO - Decay Manager stopped
2026-02-07 20:35:14 - src.pipeline - INFO - All background tasks stopped
2026-02-07 20:35:16 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 20:35:16 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 20:35:16 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 20:37:25 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 20:37:25 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 20:37:25 - __main__ - INFO - Storage directory ready: data
2026-02-07 20:37:25 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 20:37:25 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 20:37:26 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 20:37:26 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 20:37:26 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 20:37:32 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 20:37:33 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 20:37:34 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 20:37:34 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 20:37:34 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 20:37:44 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 20:37:44 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 20:37:44 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 20:37:44 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 20:37:44 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:37:44 - src.pipeline - INFO - Starting background tasks...
2026-02-07 20:37:44 - src.pipeline - INFO - Reflection loop started
2026-02-07 20:37:44 - src.pipeline - INFO - Decay manager started
2026-02-07 20:37:44 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 20:37:44 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:37:44 - __main__ - INFO - Creating Gradio web interface...
2026-02-07 20:37:44 - src.gradio_ui - INFO - New conversation started with session_id: 58cd48fa-729e-4908-9baa-d01674ffc9d0
2026-02-07 20:37:44 - src.gradio_ui - INFO - GradioUI initialized
2026-02-07 20:37:44 - __main__ - INFO - Gradio interface created successfully
2026-02-07 20:37:44 - src.gradio_ui - INFO - Creating Gradio interface...
2026-02-07 20:37:47 - src.gradio_ui - INFO - Launching Gradio interface on 127.0.0.1:7861
2026-02-07 20:38:27 - src.gradio_ui - INFO - Processing user message: Hi
...
2026-02-07 20:38:27 - src.pipeline - INFO - Processing turn 1: Hi...
2026-02-07 20:38:41 - src.memory_analyzer - WARNING - No JSON found in SLM response, using fallback
2026-02-07 20:38:41 - src.memory_analyzer - INFO - Creating fallback extraction for failed analysis
2026-02-07 20:38:42 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1452.88ms: 0 memories, 0 tokens
2026-02-07 20:38:47 - src.pipeline - INFO - Response generated: How can I assist you today?...
2026-02-07 20:38:47 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 20:38:47 - src.gradio_ui - INFO - Response generated and added to history
2026-02-07 20:46:02 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 20:46:02 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 20:46:02 - __main__ - INFO - Storage directory ready: data
2026-02-07 20:46:02 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 20:46:02 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 20:46:02 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 20:46:02 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 20:46:02 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 20:46:03 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 20:46:05 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 20:46:05 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 20:46:05 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 20:46:05 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 20:46:07 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 20:46:07 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 20:46:07 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 20:46:07 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 20:46:07 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:46:07 - src.pipeline - INFO - Starting background tasks...
2026-02-07 20:46:07 - src.pipeline - INFO - Reflection loop started
2026-02-07 20:46:07 - src.pipeline - INFO - Decay manager started
2026-02-07 20:46:07 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 20:46:07 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:46:07 - __main__ - INFO - Creating Gradio web interface...
2026-02-07 20:46:07 - src.gradio_ui - INFO - New conversation started with session_id: 156aa34c-6fb4-4e8b-ad8a-90d53a503786
2026-02-07 20:46:07 - src.gradio_ui - INFO - GradioUI initialized
2026-02-07 20:46:07 - __main__ - INFO - Gradio interface created successfully
2026-02-07 20:46:07 - src.gradio_ui - INFO - Creating Gradio interface...
2026-02-07 20:46:07 - src.gradio_ui - INFO - Launching Gradio interface on 127.0.0.1:7861
2026-02-07 20:46:21 - src.gradio_ui - INFO - Processing user message: Hi
...
2026-02-07 20:46:21 - src.pipeline - INFO - Processing turn 1: Hi...
2026-02-07 20:46:32 - src.memory_analyzer - WARNING - No JSON found in SLM response, using fallback
2026-02-07 20:46:32 - src.memory_analyzer - INFO - Creating fallback extraction for failed analysis
2026-02-07 20:46:34 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1539.95ms: 0 memories, 0 tokens
2026-02-07 20:46:39 - src.pipeline - INFO - Response generated: How can I assist you today?...
2026-02-07 20:46:39 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 20:46:39 - src.gradio_ui - INFO - Response generated and added to history
2026-02-07 20:51:28 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 20:51:28 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 20:51:28 - __main__ - INFO - Storage directory ready: data
2026-02-07 20:51:28 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 20:51:28 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 20:51:28 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 20:51:28 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 20:51:28 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 20:51:29 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 20:51:30 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 20:51:31 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 20:51:31 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 20:51:31 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 20:51:32 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 20:51:32 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 20:51:32 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 20:51:32 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 20:51:32 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:51:32 - src.pipeline - INFO - Starting background tasks...
2026-02-07 20:51:32 - src.pipeline - INFO - Reflection loop started
2026-02-07 20:51:32 - src.pipeline - INFO - Decay manager started
2026-02-07 20:51:32 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 20:51:32 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:51:32 - __main__ - INFO - Creating Gradio web interface...
2026-02-07 20:51:32 - src.gradio_ui - INFO - New conversation started with session_id: bd7bd937-0593-460b-92ee-0c5943de9c75
2026-02-07 20:51:32 - src.gradio_ui - INFO - GradioUI initialized
2026-02-07 20:51:32 - __main__ - INFO - Gradio interface created successfully
2026-02-07 20:51:32 - src.gradio_ui - INFO - Creating Gradio interface...
2026-02-07 20:51:32 - __main__ - ERROR - Unexpected error in main loop: Chatbot.__init__() got an unexpected keyword argument 'type'
Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 262, in async_main
    ui.launch(
  File "D:\Neurohacks\src\gradio_ui.py", line 167, in launch
    chatbot = gr.Chatbot(
              ^^^^^^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\gradio\component_meta.py", line 194, in wrapper
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Chatbot.__init__() got an unexpected keyword argument 'type'
2026-02-07 20:51:32 - src.pipeline - INFO - Stopping background tasks...
2026-02-07 20:51:32 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-07 20:51:32 - src.decay_manager - INFO - Decay Manager stopped
2026-02-07 20:51:32 - src.pipeline - INFO - All background tasks stopped
2026-02-07 20:51:34 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 20:51:34 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 20:51:34 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 20:54:01 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 20:54:01 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 20:54:01 - __main__ - INFO - Storage directory ready: data
2026-02-07 20:54:01 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 20:54:01 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 20:54:01 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 20:54:01 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 20:54:01 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 20:54:02 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 20:54:03 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 20:54:04 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 20:54:04 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 20:54:04 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 20:54:05 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 20:54:05 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 20:54:05 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 20:54:05 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 20:54:05 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:54:05 - src.pipeline - INFO - Starting background tasks...
2026-02-07 20:54:05 - src.pipeline - INFO - Reflection loop started
2026-02-07 20:54:05 - src.pipeline - INFO - Decay manager started
2026-02-07 20:54:05 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 20:54:05 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 20:54:05 - src.terminal_ui - INFO - New conversation started with session_id: f42891b4-9055-4a50-b2c7-9fad0a0e98c0
2026-02-07 20:54:05 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 20:54:13 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-07 20:54:13 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 20:54:13 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 20:54:19 - src.memory_analyzer - INFO - Parsed SLM response: category=pinned, confidence=0.80
2026-02-07 20:54:21 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1379.03ms: 0 memories, 0 tokens
2026-02-07 20:54:24 - src.pipeline - INFO - Response generated: How can I assist you today?...
2026-02-07 20:54:24 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 20:54:42 - src.pipeline - INFO - Processing turn 2: my name is ishaan but always call me jumbo...
2026-02-07 20:54:42 - src.pipeline - ERROR - Failed to store memory: GraphMemoryEngine.store_memory() takes 2 positional arguments but 3 were given
Traceback (most recent call last):
  File "D:\Neurohacks\src\pipeline.py", line 279, in _store_memory
    memory_id = await self.graph_engine.store_memory(extraction, original_text)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: GraphMemoryEngine.store_memory() takes 2 positional arguments but 3 were given
2026-02-07 20:54:49 - src.memory_analyzer - INFO - Parsed SLM response: category=discard, confidence=0.80
2026-02-07 20:54:51 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1971.20ms: 0 memories, 0 tokens
2026-02-07 20:54:52 - src.pipeline - INFO - Response generated: What's up Jumbo?...
2026-02-07 20:54:52 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-07 21:04:16 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-07 21:04:16 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-07 21:04:16 - src.pipeline - INFO - Stopping background tasks...
2026-02-07 21:04:16 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-07 21:04:16 - src.decay_manager - INFO - Decay Manager stopped
2026-02-07 21:04:16 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-07 21:04:16 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-07 21:04:16 - src.pipeline - INFO - All background tasks stopped
2026-02-07 21:04:16 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 21:04:16 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 21:04:16 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 21:04:23 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 21:04:23 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 21:04:23 - __main__ - INFO - Storage directory ready: data
2026-02-07 21:04:23 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 21:04:23 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 21:04:23 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 21:04:23 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 21:04:23 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 21:04:23 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 21:04:25 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 21:04:25 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 21:04:25 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 21:04:25 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 21:04:27 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 21:04:27 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 21:04:27 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 21:04:27 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 21:04:27 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 21:04:27 - src.pipeline - INFO - Starting background tasks...
2026-02-07 21:04:27 - src.pipeline - INFO - Reflection loop started
2026-02-07 21:04:27 - src.pipeline - INFO - Decay manager started
2026-02-07 21:04:27 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 21:04:27 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 21:04:27 - src.terminal_ui - INFO - New conversation started with session_id: 6c2eed69-4bfc-4c75-8b26-29395e8fae06
2026-02-07 21:04:27 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 21:04:30 - src.pipeline - INFO - Processing turn 1: HI...
2026-02-07 21:04:30 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 21:04:30 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 21:04:35 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.80
2026-02-07 21:04:37 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1395.33ms: 0 memories, 0 tokens
2026-02-07 21:04:41 - src.pipeline - INFO - Response generated: Hello! How can I assist you today?...
2026-02-07 21:04:41 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 21:05:04 - src.pipeline - INFO - Processing turn 2: My favourite programming language is Rust. Remembe...
2026-02-07 21:05:05 - src.pipeline - INFO - Memory stored successfully: id=579cf72f-e501-4405-aebb-4c61503dadaf
2026-02-07 21:05:10 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 21:05:11 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1338.92ms: 0 memories, 0 tokens
2026-02-07 21:05:14 - src.pipeline - INFO - Response generated: I've taken note that Rust is your preferred progra...
2026-02-07 21:05:14 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-07 21:05:33 - src.pipeline - INFO - Processing turn 3: Tomorrow I have an ML exam....
2026-02-07 21:05:34 - src.pipeline - INFO - Memory stored successfully: id=11d7fc36-9970-494f-b067-962959817680
2026-02-07 21:05:39 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 21:05:40 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1257.89ms: 0 memories, 0 tokens
2026-02-07 21:05:48 - src.pipeline - INFO - Response generated: Good luck with your ML exam tomorrow. Is there any...
2026-02-07 21:05:48 - src.pipeline - INFO - Turn 3 completed successfully
2026-02-07 21:06:09 - src.pipeline - INFO - Processing turn 4: Actually I changed my favourite language from RUST...
2026-02-07 21:06:10 - src.pipeline - INFO - Memory stored successfully: id=4977cac4-fa62-485b-8bab-68ccc8f503d5
2026-02-07 21:06:15 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 21:06:16 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1338.09ms: 0 memories, 0 tokens
2026-02-07 21:06:21 - src.pipeline - INFO - Response generated: It looks like you've made a switch to Python. That...
2026-02-07 21:06:21 - src.pipeline - INFO - Turn 4 completed successfully
2026-02-07 21:16:21 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-07 21:16:21 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-07 21:16:21 - src.pipeline - INFO - Stopping background tasks...
2026-02-07 21:16:21 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-07 21:16:21 - src.decay_manager - INFO - Decay Manager stopped
2026-02-07 21:16:22 - src.pipeline - INFO - Memory stored successfully: id=5bacaffc-6e1f-40a3-af38-5597c0cd9175
2026-02-07 21:16:22 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-07 21:16:22 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-07 21:16:22 - src.pipeline - INFO - All background tasks stopped
2026-02-07 21:16:22 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 21:16:22 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 21:16:23 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 21:16:29 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 21:16:29 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 21:16:29 - __main__ - INFO - Storage directory ready: data
2026-02-07 21:16:29 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 21:16:29 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 21:16:29 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 21:16:29 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 21:16:29 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 21:16:30 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 21:16:31 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 21:16:31 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 21:16:31 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 21:16:31 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 21:16:33 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 21:16:33 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 21:16:33 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 21:16:33 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 21:16:33 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 21:16:33 - src.pipeline - INFO - Starting background tasks...
2026-02-07 21:16:33 - src.pipeline - INFO - Reflection loop started
2026-02-07 21:16:33 - src.pipeline - INFO - Decay manager started
2026-02-07 21:16:33 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 21:16:33 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 21:16:33 - src.terminal_ui - INFO - New conversation started with session_id: b835d94c-2703-443c-a8cb-6dbde9b19762
2026-02-07 21:16:33 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 21:16:37 - src.pipeline - INFO - Processing turn 1: what is my name?...
2026-02-07 21:16:37 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 21:16:37 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 21:16:42 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.80
2026-02-07 21:16:44 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1485.75ms: 0 memories, 0 tokens
2026-02-07 21:16:48 - src.pipeline - INFO - Response generated: I don't have that information....
2026-02-07 21:16:48 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 21:18:30 - src.pipeline - INFO - Processing turn 2: my name is jojo remmeber that...
2026-02-07 21:18:31 - src.pipeline - INFO - Memory stored successfully: id=df6d51ae-634e-48db-bd0a-972bca4dafc8
2026-02-07 21:18:36 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.80
2026-02-07 21:18:37 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1382.10ms: 0 memories, 0 tokens
2026-02-07 21:18:41 - src.pipeline - INFO - Response generated: I don't have the ability to remember or keep track...
2026-02-07 21:18:41 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-07 21:19:26 - src.pipeline - INFO - Processing turn 3: hi bro...
2026-02-07 21:19:27 - src.pipeline - INFO - Memory stored successfully: id=3b547b49-9274-47bb-8891-3572b7f7de07
2026-02-07 21:19:32 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.50
2026-02-07 21:19:33 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1338.28ms: 0 memories, 0 tokens
2026-02-07 21:19:36 - src.pipeline - INFO - Response generated: It's nice to chat with you, but I don't have a per...
2026-02-07 21:19:36 - src.pipeline - INFO - Turn 3 completed successfully
2026-02-07 21:23:34 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-07 21:23:34 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-07 21:23:34 - src.pipeline - INFO - Stopping background tasks...
2026-02-07 21:23:34 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-07 21:23:34 - src.decay_manager - INFO - Decay Manager stopped
2026-02-07 21:23:34 - src.pipeline - INFO - Memory stored successfully: id=70254902-5c8e-4298-a555-fc987da59a6b
2026-02-07 21:23:35 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-07 21:23:35 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-07 21:23:35 - src.pipeline - INFO - All background tasks stopped
2026-02-07 21:23:35 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 21:23:35 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 21:23:35 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 21:31:28 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 21:31:28 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 21:31:28 - __main__ - INFO - Storage directory ready: data
2026-02-07 21:31:28 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 21:31:28 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 21:31:28 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 21:31:28 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 21:31:28 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 21:31:28 - src.embedding_service - ERROR - Failed to load embedding model: Model path does not exist: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 21:31:28 - __main__ - ERROR - Failed to initialize pipeline: Embedding model initialization failed: Model path does not exist: ./data/models/mxbai-embed-large-v1_fp32.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\embedding_service.py", line 41, in _load_model
    self.model = Llama(
                 ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 370, in __init__
    raise ValueError(f"Model path does not exist: {model_path}")
ValueError: Model path does not exist: ./data/models/mxbai-embed-large-v1_fp32.gguf

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 131, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 76, in __init__
    self.embedding_service = EmbeddingService(config.embedding)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\embedding_service.py", line 36, in __init__
    self._load_model()
  File "D:\Neurohacks\src\embedding_service.py", line 51, in _load_model
    raise RuntimeError(f"Embedding model initialization failed: {e}")
RuntimeError: Embedding model initialization failed: Model path does not exist: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 21:31:28 - __main__ - ERROR - Unexpected error in main loop: Embedding model initialization failed: Model path does not exist: ./data/models/mxbai-embed-large-v1_fp32.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\embedding_service.py", line 41, in _load_model
    self.model = Llama(
                 ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 370, in __init__
    raise ValueError(f"Model path does not exist: {model_path}")
ValueError: Model path does not exist: ./data/models/mxbai-embed-large-v1_fp32.gguf

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 216, in async_main
    pipeline = await initialize_pipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\main.py", line 131, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 76, in __init__
    self.embedding_service = EmbeddingService(config.embedding)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\embedding_service.py", line 36, in __init__
    self._load_model()
  File "D:\Neurohacks\src\embedding_service.py", line 51, in _load_model
    raise RuntimeError(f"Embedding model initialization failed: {e}")
RuntimeError: Embedding model initialization failed: Model path does not exist: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 21:31:28 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 21:48:38 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 21:48:38 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 21:48:38 - __main__ - INFO - Storage directory ready: data
2026-02-07 21:48:38 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 21:48:38 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 21:48:38 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 21:48:38 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 21:48:38 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 21:48:38 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 21:48:38 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 21:48:39 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 21:48:40 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 21:48:40 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 21:48:40 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 21:48:40 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 21:48:42 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 1/3): Failed to create llama_context
2026-02-07 21:48:42 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 2/3)
2026-02-07 21:48:43 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 2/3): Failed to create llama_context
2026-02-07 21:48:43 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 3/3)
2026-02-07 21:48:44 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 3/3): Failed to create llama_context
2026-02-07 21:48:44 - __main__ - ERROR - Failed to initialize pipeline: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 131, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-07 21:48:44 - __main__ - ERROR - Unexpected error in main loop: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 216, in async_main
    pipeline = await initialize_pipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\main.py", line 131, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-07 21:48:44 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 21:48:44 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 21:48:44 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-07 22:44:56 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 22:44:56 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 22:44:56 - __main__ - INFO - Storage directory ready: data
2026-02-07 22:44:56 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 22:44:56 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 22:44:56 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 22:44:56 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 22:44:56 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 22:44:57 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 22:44:57 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 22:44:57 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 22:44:59 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 22:44:59 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 22:44:59 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 22:44:59 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 22:45:01 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 22:45:01 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 22:45:01 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 22:45:01 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 22:45:01 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 22:45:01 - src.pipeline - INFO - Starting background tasks...
2026-02-07 22:45:01 - src.pipeline - INFO - Reflection loop started
2026-02-07 22:45:01 - src.pipeline - INFO - Decay manager started
2026-02-07 22:45:01 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 22:45:01 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 22:45:01 - src.terminal_ui - INFO - New conversation started with session_id: ff298a9a-e823-439d-b748-f81b3c13e918
2026-02-07 22:45:01 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 22:45:19 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-07 22:45:19 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 22:45:19 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 22:45:23 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.00
2026-02-07 22:45:25 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1747.76ms: 10 memories, 265 tokens
2026-02-07 22:45:30 - src.pipeline - INFO - Response generated: Hello! How can I assist you today?...
2026-02-07 22:45:30 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 22:46:28 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-07 22:46:28 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-07 22:46:28 - src.pipeline - INFO - Stopping background tasks...
2026-02-07 22:46:28 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-07 22:46:28 - src.decay_manager - INFO - Decay Manager stopped
2026-02-07 22:46:28 - src.pipeline - ERROR - Failed to store PINNED memory: 'PinnedMemoryManager' object has no attribute 'add'
Traceback (most recent call last):
  File "D:\Neurohacks\src\pipeline.py", line 302, in _store_pinned_memory
    self.pinned_mgr.add(category_key, key_info)
    ^^^^^^^^^^^^^^^^^^^
AttributeError: 'PinnedMemoryManager' object has no attribute 'add'
2026-02-07 22:46:28 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-07 22:46:28 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-07 22:46:28 - src.pipeline - INFO - All background tasks stopped
2026-02-07 22:46:28 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 22:46:28 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 22:46:28 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-07 22:46:28 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 22:47:23 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 22:47:23 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 22:47:23 - __main__ - INFO - Storage directory ready: data
2026-02-07 22:47:23 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 22:47:23 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 22:47:23 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 22:47:23 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 22:47:23 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 22:47:23 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 22:47:23 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 22:47:24 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 22:47:25 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 22:47:26 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 22:47:26 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 22:47:26 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 22:47:27 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 22:47:27 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 22:47:27 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 22:47:27 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 22:47:27 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 22:47:27 - src.pipeline - INFO - Starting background tasks...
2026-02-07 22:47:27 - src.pipeline - INFO - Reflection loop started
2026-02-07 22:47:27 - src.pipeline - INFO - Decay manager started
2026-02-07 22:47:27 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 22:47:27 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 22:47:27 - src.terminal_ui - INFO - New conversation started with session_id: 715ae68e-bee3-4c20-8688-1fa390cfd223
2026-02-07 22:47:27 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 22:47:36 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-07 22:47:36 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 22:47:36 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 22:47:39 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.00
2026-02-07 22:47:41 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1610.87ms: 10 memories, 265 tokens
2026-02-07 22:47:44 - src.pipeline - INFO - Response generated: Hello! How can I assist you today?...
2026-02-07 22:47:44 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 22:49:25 - src.pipeline - INFO - Processing turn 2: My name is Alex...
2026-02-07 22:49:25 - src.pipeline - INFO - PINNED memory stored in category 'preferences': hi...
2026-02-07 22:49:31 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 22:49:32 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1072.16ms: 10 memories, 266 tokens
2026-02-07 22:49:37 - src.pipeline - INFO - Response generated: Nice to meet you, Alex. Is there something on your...
2026-02-07 22:49:37 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-07 22:52:32 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-07 22:52:32 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-07 22:52:32 - src.pipeline - INFO - Stopping background tasks...
2026-02-07 22:52:32 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-07 22:52:32 - src.decay_manager - INFO - Decay Manager stopped
2026-02-07 22:52:32 - src.pipeline - INFO - PINNED memory stored in category 'identity': My name is Alex...
2026-02-07 22:52:32 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-07 22:52:32 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-07 22:52:32 - src.pipeline - INFO - All background tasks stopped
2026-02-07 22:52:32 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 22:52:32 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 22:52:32 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-07 22:52:32 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 22:54:06 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 22:54:06 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 22:54:06 - __main__ - INFO - Storage directory ready: data
2026-02-07 22:54:06 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 22:54:06 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 22:54:06 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 22:54:06 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 22:54:06 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 22:54:06 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 22:54:06 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 22:54:07 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 22:54:10 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 22:54:10 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 22:54:10 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 22:54:10 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 22:54:11 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 22:54:11 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 22:54:11 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 22:54:11 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 22:54:11 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 22:54:11 - src.pipeline - INFO - Starting background tasks...
2026-02-07 22:54:11 - src.pipeline - INFO - Reflection loop started
2026-02-07 22:54:11 - src.pipeline - INFO - Decay manager started
2026-02-07 22:54:11 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 22:54:11 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 22:54:11 - src.terminal_ui - INFO - New conversation started with session_id: 536356e1-4071-4df0-9bca-bdacdf6f7567
2026-02-07 22:54:11 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 22:54:11 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 22:54:11 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 22:54:17 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-07 22:54:22 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.00
2026-02-07 22:54:24 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2076.97ms: 10 memories, 271 tokens
2026-02-07 22:54:29 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-07 22:54:29 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 22:54:29 - src.pipeline - ERROR - Failed to store PINNED memory: 'PinnedMemoryManager' object has no attribute 'add'
Traceback (most recent call last):
  File "D:\Neurohacks\src\pipeline.py", line 302, in _store_pinned_memory
    self.pinned_mgr.add(category_key, key_info)
    ^^^^^^^^^^^^^^^^^^^
AttributeError: 'PinnedMemoryManager' object has no attribute 'add'
2026-02-07 22:55:08 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-07 22:55:08 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-07 22:55:08 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-07 22:55:08 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 22:55:08 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 22:55:08 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-07 22:55:08 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 22:55:14 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 22:55:14 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 22:55:14 - __main__ - INFO - Storage directory ready: data
2026-02-07 22:55:14 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 22:55:14 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 22:55:14 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 22:55:14 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 22:55:14 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 22:55:14 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 22:55:14 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 22:55:14 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 22:55:16 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 22:55:16 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 22:55:16 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 22:55:16 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 22:55:17 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 22:55:17 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 22:55:17 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 22:55:17 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 22:55:17 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 22:55:17 - src.pipeline - INFO - Starting background tasks...
2026-02-07 22:55:17 - src.pipeline - INFO - Reflection loop started
2026-02-07 22:55:17 - src.pipeline - INFO - Decay manager started
2026-02-07 22:55:17 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 22:55:17 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 22:55:17 - src.terminal_ui - INFO - New conversation started with session_id: 51971a69-ec27-40b1-943a-4fb54461cfff
2026-02-07 22:55:17 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 22:55:19 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-07 22:55:19 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 22:55:19 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 22:55:24 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.00
2026-02-07 22:55:25 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1572.72ms: 10 memories, 271 tokens
2026-02-07 22:55:30 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-07 22:55:30 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 22:55:41 - src.pipeline - INFO - Processing turn 2: my name is jojo...
2026-02-07 22:55:41 - src.pipeline - ERROR - Failed to store PINNED memory: 'PinnedMemoryManager' object has no attribute 'add'
Traceback (most recent call last):
  File "D:\Neurohacks\src\pipeline.py", line 302, in _store_pinned_memory
    self.pinned_mgr.add(category_key, key_info)
    ^^^^^^^^^^^^^^^^^^^
AttributeError: 'PinnedMemoryManager' object has no attribute 'add'
2026-02-07 22:55:48 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 22:55:49 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1194.32ms: 10 memories, 271 tokens
2026-02-07 22:55:52 - src.pipeline - INFO - Response generated: Nice to meet you, Jojo. Is there something I can h...
2026-02-07 22:55:52 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-07 22:57:17 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 22:57:17 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 22:57:17 - __main__ - INFO - Storage directory ready: data
2026-02-07 22:57:17 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 22:57:17 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 22:57:17 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 22:57:17 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 22:57:17 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 22:57:17 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 22:57:17 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 22:57:18 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 22:57:19 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 22:57:19 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 22:57:19 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 22:57:19 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 22:57:20 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 22:57:20 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 22:57:20 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 22:57:20 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 22:57:20 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 22:57:20 - src.pipeline - INFO - Starting background tasks...
2026-02-07 22:57:20 - src.pipeline - INFO - Reflection loop started
2026-02-07 22:57:20 - src.pipeline - INFO - Decay manager started
2026-02-07 22:57:20 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 22:57:20 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 22:57:20 - src.terminal_ui - INFO - New conversation started with session_id: b1eb063f-6414-4939-b15c-8719d1fda5f0
2026-02-07 22:57:20 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 22:57:20 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 22:57:20 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 22:58:05 - src.pipeline - INFO - Processing turn 1: My name is Jojo...
2026-02-07 22:58:10 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.80
2026-02-07 22:58:12 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1735.11ms: 2 memories, 65 tokens
2026-02-07 22:58:17 - src.pipeline - INFO - Response generated: Nice to meet you, Jojo. Is there anything I can he...
2026-02-07 22:58:17 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 22:58:17 - src.pipeline - INFO - PINNED memory stored in category 'identity': My name is Jojo...
2026-02-07 22:59:22 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-07 22:59:22 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-07 22:59:22 - src.pipeline - INFO - Stopping background tasks...
2026-02-07 22:59:22 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-07 22:59:22 - src.decay_manager - INFO - Decay Manager stopped
2026-02-07 22:59:22 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-07 22:59:22 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-07 22:59:22 - src.pipeline - INFO - All background tasks stopped
2026-02-07 22:59:23 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 22:59:23 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 22:59:23 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-07 22:59:23 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 23:00:33 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 23:00:33 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 23:00:33 - __main__ - INFO - Storage directory ready: data
2026-02-07 23:00:33 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 23:00:33 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 23:00:33 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 23:00:33 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 23:00:33 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 23:00:33 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 23:00:33 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 23:00:34 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 23:00:35 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 23:00:35 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 23:00:35 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 23:00:35 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 23:00:36 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 23:00:36 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 23:00:36 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 23:00:36 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 23:00:36 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:00:36 - src.pipeline - INFO - Starting background tasks...
2026-02-07 23:00:36 - src.pipeline - INFO - Reflection loop started
2026-02-07 23:00:36 - src.pipeline - INFO - Decay manager started
2026-02-07 23:00:36 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 23:00:36 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:00:36 - src.terminal_ui - INFO - New conversation started with session_id: d90bc035-f745-4061-a8bf-c9ed4eeb7b96
2026-02-07 23:00:36 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 23:00:36 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 23:00:36 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 23:00:50 - src.pipeline - INFO - Processing turn 1: I am the superman...
2026-02-07 23:00:54 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:00:56 - src.graph_engine - WARNING - Vector search failed, falling back to confidence-based retrieval: {neo4j_code: Neo.ClientError.Statement.TypeError} {message: Index query vector has a dimensionality of 1024, but provided vector has 384.} {gql_status: 52N37} {gql_status_description: error: procedure exception - procedure execution error. Execution of the procedure db.index.vector.queryNodes() failed.}
2026-02-07 23:00:57 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `CONTRADICTS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=73, offset=73>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 73, 'line': 2, 'column': 73}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-07 23:00:57 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `RELATES_TO` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=85, offset=85>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 85, 'line': 2, 'column': 85}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-07 23:00:57 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `REPLACES` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=64, offset=64>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 64, 'line': 2, 'column': 64}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-07 23:00:57 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N52', status_description='warn: property key does not exist. The property `weight` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=4, column=68, offset=236>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 236, 'line': 4, 'column': 68}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-07 23:00:57 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N52', status_description='warn: property key does not exist. The property `weight` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=7, column=81, offset=434>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 434, 'line': 7, 'column': 81}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-07 23:00:57 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `STRENGTHENS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=52, offset=52>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 52, 'line': 2, 'column': 52}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-07 23:00:57 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2343.67ms: 10 memories, 271 tokens
2026-02-07 23:01:04 - src.pipeline - INFO - Response generated: It's not every day I get to interact with a hero o...
2026-02-07 23:01:04 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 23:01:04 - src.pipeline - INFO - PINNED memory stored in category 'preferences': I am the superman...
2026-02-07 23:01:11 - src.pipeline - INFO - Processing turn 2: yoyo...
2026-02-07 23:01:16 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.50
2026-02-07 23:01:17 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1166.09ms: 10 memories, 275 tokens
2026-02-07 23:01:22 - src.pipeline - INFO - Response generated: A simple yet intriguing request. It seems like a c...
2026-02-07 23:01:22 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-07 23:01:22 - src.pipeline - INFO - PINNED memory stored in category 'preferences': yoyo...
2026-02-07 23:09:35 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-07 23:09:35 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-07 23:09:35 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-07 23:09:35 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 23:09:35 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 23:09:35 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-07 23:09:36 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 23:09:40 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 23:09:40 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 23:09:40 - __main__ - INFO - Storage directory ready: data
2026-02-07 23:09:40 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 23:09:40 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 23:09:40 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 23:09:40 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 23:09:40 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 23:09:40 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 23:09:40 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 23:09:41 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 23:09:42 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 23:09:42 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 23:09:42 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 23:09:42 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 23:09:43 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 23:09:43 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 23:09:43 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 23:09:43 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 23:09:43 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:09:43 - src.pipeline - INFO - Starting background tasks...
2026-02-07 23:09:43 - src.pipeline - INFO - Reflection loop started
2026-02-07 23:09:43 - src.pipeline - INFO - Decay manager started
2026-02-07 23:09:43 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 23:09:43 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:09:43 - src.terminal_ui - INFO - New conversation started with session_id: e6b6db7c-7681-4613-bbde-6ca789389102
2026-02-07 23:09:43 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 23:09:43 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 23:09:43 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 23:09:51 - src.pipeline - INFO - Processing turn 1: I like the brother bear very much...
2026-02-07 23:09:55 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:09:57 - src.graph_engine - WARNING - Vector search failed, falling back to confidence-based retrieval: {neo4j_code: Neo.ClientError.Statement.TypeError} {message: Index query vector has a dimensionality of 1024, but provided vector has 384.} {gql_status: 52N37} {gql_status_description: error: procedure exception - procedure execution error. Execution of the procedure db.index.vector.queryNodes() failed.}
2026-02-07 23:09:58 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `CONTRADICTS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=73, offset=73>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 73, 'line': 2, 'column': 73}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-07 23:09:58 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `RELATES_TO` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=85, offset=85>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 85, 'line': 2, 'column': 85}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-07 23:09:58 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `REPLACES` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=64, offset=64>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 64, 'line': 2, 'column': 64}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-07 23:09:58 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N52', status_description='warn: property key does not exist. The property `weight` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=4, column=68, offset=236>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 236, 'line': 4, 'column': 68}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-07 23:09:58 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N52', status_description='warn: property key does not exist. The property `weight` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=7, column=81, offset=434>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 434, 'line': 7, 'column': 81}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-07 23:09:58 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `STRENGTHENS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=52, offset=52>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 52, 'line': 2, 'column': 52}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-07 23:09:58 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2405.23ms: 10 memories, 271 tokens
2026-02-07 23:10:07 - src.pipeline - INFO - Response generated: It's a beautifully animated film with a powerful m...
2026-02-07 23:10:07 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 23:10:07 - src.pipeline - INFO - PINNED memory stored in category 'preferences': I like the brother bear very much...
2026-02-07 23:10:08 - src.reflection_loop - ERROR - Failed to increase confidence: Existing exports of data: object cannot be re-sized
2026-02-07 23:10:12 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-07 23:10:12 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-07 23:10:12 - src.pipeline - INFO - Stopping background tasks...
2026-02-07 23:10:12 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-07 23:10:12 - src.decay_manager - INFO - Decay Manager stopped
2026-02-07 23:10:12 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-07 23:10:12 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-07 23:10:12 - src.pipeline - INFO - All background tasks stopped
2026-02-07 23:10:12 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 23:10:12 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 23:10:12 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-07 23:10:12 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 23:13:20 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 23:13:20 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 23:13:20 - __main__ - INFO - Storage directory ready: data
2026-02-07 23:13:20 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 23:13:20 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 23:13:20 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 23:13:20 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 23:13:20 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 23:13:21 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 23:13:21 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 23:13:21 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 23:13:24 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 23:13:25 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 23:13:25 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 23:13:25 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 23:13:27 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 23:13:27 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 23:13:27 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 23:13:27 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 23:13:27 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:13:27 - src.pipeline - INFO - Starting background tasks...
2026-02-07 23:13:27 - src.pipeline - INFO - Reflection loop started
2026-02-07 23:13:27 - src.pipeline - INFO - Decay manager started
2026-02-07 23:13:27 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 23:13:27 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:13:27 - src.terminal_ui - INFO - New conversation started with session_id: 99564b15-d6b3-4075-8166-f14f3e8b4e69
2026-02-07 23:13:27 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 23:13:27 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 23:13:27 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 23:13:41 - src.pipeline - INFO - Processing turn 1: hi i love the movie incredibles very mu h...
2026-02-07 23:13:46 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:13:49 - src.retrieval_gatekeeper - INFO - Retrieval completed in 3007.26ms: 0 memories, 14 tokens
2026-02-07 23:13:58 - src.pipeline - INFO - Response generated: That's great to hear. The Incredibles is a classic...
2026-02-07 23:13:58 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 23:13:58 - src.pipeline - INFO - PINNED memory stored in category 'preferences': hi i love the movie incredibles very mu h...
2026-02-07 23:14:25 - src.pipeline - INFO - Processing turn 2: what is my name...
2026-02-07 23:14:29 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:14:31 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1809.18ms: 2 memories, 75 tokens
2026-02-07 23:14:35 - src.pipeline - INFO - Response generated: I don't have any information about your name. Our ...
2026-02-07 23:14:35 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-07 23:14:35 - src.pipeline - INFO - PINNED memory stored in category 'identity': what is my name...
2026-02-07 23:25:31 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-07 23:25:31 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-07 23:25:31 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-07 23:25:31 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 23:25:31 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 23:25:31 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-07 23:25:31 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 23:25:38 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 23:25:38 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 23:25:38 - __main__ - INFO - Storage directory ready: data
2026-02-07 23:25:38 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 23:25:38 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 23:25:38 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 23:25:38 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 23:25:38 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 23:25:38 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 23:25:38 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 23:25:39 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 23:25:40 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 23:25:41 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 23:25:41 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 23:25:41 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 23:25:42 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 23:25:42 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 23:25:42 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 23:25:42 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 23:25:42 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:25:42 - src.pipeline - INFO - Starting background tasks...
2026-02-07 23:25:42 - src.pipeline - INFO - Reflection loop started
2026-02-07 23:25:42 - src.pipeline - INFO - Decay manager started
2026-02-07 23:25:42 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 23:25:42 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:25:42 - src.terminal_ui - INFO - New conversation started with session_id: 327e2828-7239-4c0b-a6bb-f671d9fec8f1
2026-02-07 23:25:42 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 23:25:42 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 23:25:42 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 23:26:01 - src.pipeline - INFO - Processing turn 1: my name is superdomo...
2026-02-07 23:26:06 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.85
2026-02-07 23:26:08 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1725.61ms: 2 memories, 75 tokens
2026-02-07 23:26:13 - src.pipeline - INFO - Response generated: It's nice to meet you, Superdomo. Is there somethi...
2026-02-07 23:26:13 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 23:26:13 - src.pipeline - INFO - PINNED memory stored in category 'identity': my name is superdomo...
2026-02-07 23:26:18 - src.pipeline - INFO - Processing turn 2: what is my name?...
2026-02-07 23:26:22 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:26:24 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1791.72ms: 4 memories, 134 tokens
2026-02-07 23:26:25 - src.pipeline - INFO - Response generated: Your name is Superdomo....
2026-02-07 23:26:25 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-07 23:26:25 - src.pipeline - INFO - PINNED memory stored in category 'identity': what is my name?...
2026-02-07 23:26:37 - src.pipeline - INFO - Processing turn 3: what is my name...
2026-02-07 23:26:42 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:26:44 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1788.83ms: 4 memories, 134 tokens
2026-02-07 23:26:45 - src.pipeline - INFO - Response generated: You've mentioned it before. Your name is Superdomo...
2026-02-07 23:26:45 - src.pipeline - INFO - Turn 3 completed successfully
2026-02-07 23:26:45 - src.pipeline - INFO - PINNED memory stored in category 'identity': what is my name...
2026-02-07 23:27:44 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 23:27:44 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 23:27:44 - __main__ - INFO - Storage directory ready: data
2026-02-07 23:27:44 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 23:27:44 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 23:27:44 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 23:27:44 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 23:27:44 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 23:27:44 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 23:27:44 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 23:27:45 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 23:27:46 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 23:27:47 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 23:27:47 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 23:27:47 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 23:27:48 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 23:27:48 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 23:27:48 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 23:27:48 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 23:27:48 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:27:48 - src.pipeline - INFO - Starting background tasks...
2026-02-07 23:27:48 - src.pipeline - INFO - Reflection loop started
2026-02-07 23:27:48 - src.pipeline - INFO - Decay manager started
2026-02-07 23:27:48 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 23:27:48 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:27:48 - src.terminal_ui - INFO - New conversation started with session_id: 334141b3-cfb3-441f-92ce-1d0082095d90
2026-02-07 23:27:48 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 23:27:48 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 23:27:48 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 23:28:07 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-07 23:28:12 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.00
2026-02-07 23:28:13 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1657.73ms: 10 memories, 281 tokens
2026-02-07 23:28:18 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-07 23:28:18 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 23:28:18 - src.pipeline - INFO - PINNED memory stored in category 'preferences': hi...
2026-02-07 23:28:28 - src.pipeline - INFO - Processing turn 2: my name is my my name is kraken...
2026-02-07 23:28:33 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.80
2026-02-07 23:28:35 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1334.86ms: 10 memories, 283 tokens
2026-02-07 23:28:37 - src.pipeline - INFO - Response generated: Nice to meet you, Kraken. Is there something I can...
2026-02-07 23:28:37 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-07 23:28:37 - src.pipeline - INFO - PINNED memory stored in category 'identity': my name is my my name is kraken...
2026-02-07 23:29:10 - src.pipeline - INFO - Processing turn 3: what is your name...
2026-02-07 23:29:15 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:29:16 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1171.65ms: 10 memories, 293 tokens
2026-02-07 23:29:19 - src.pipeline - INFO - Response generated: I don't have a personal name. I'm an AI assistant ...
2026-02-07 23:29:19 - src.pipeline - INFO - Turn 3 completed successfully
2026-02-07 23:29:19 - src.pipeline - INFO - PINNED memory stored in category 'identity': what is your name...
2026-02-07 23:29:48 - src.pipeline - INFO - Processing turn 4: your name is jojo from now on...
2026-02-07 23:29:53 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:29:54 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1232.49ms: 10 memories, 298 tokens
2026-02-07 23:29:57 - src.pipeline - INFO - Response generated: From now on, I'll be referring to myself as Jojo. ...
2026-02-07 23:29:57 - src.pipeline - INFO - Turn 4 completed successfully
2026-02-07 23:29:57 - src.pipeline - INFO - PINNED memory stored in category 'identity': your name is jojo from now on...
2026-02-07 23:30:25 - src.pipeline - INFO - Processing turn 5: My name is kroko...
2026-02-07 23:30:30 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.80
2026-02-07 23:30:31 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1277.08ms: 10 memories, 307 tokens
2026-02-07 23:30:35 - src.pipeline - INFO - Response generated: So, to recap, we have Kraken and Kroko. I'm still ...
2026-02-07 23:30:35 - src.pipeline - INFO - Turn 5 completed successfully
2026-02-07 23:30:35 - src.pipeline - INFO - PINNED memory stored in category 'identity': My name is kroko...
2026-02-07 23:30:40 - src.pipeline - INFO - Processing turn 6: What is my name...
2026-02-07 23:30:44 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:30:45 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1144.56ms: 10 memories, 313 tokens
2026-02-07 23:30:47 - src.pipeline - INFO - Response generated: Kroko, your name is Kroko....
2026-02-07 23:30:47 - src.pipeline - INFO - Turn 6 completed successfully
2026-02-07 23:30:47 - src.pipeline - INFO - PINNED memory stored in category 'identity': What is my name...
2026-02-07 23:34:40 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 23:34:40 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 23:34:40 - __main__ - INFO - Storage directory ready: data
2026-02-07 23:34:40 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 23:34:40 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 23:34:40 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 23:34:40 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 23:34:40 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 23:34:40 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 23:34:40 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 23:34:41 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 23:34:42 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 23:34:43 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 23:34:43 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 23:34:43 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 23:34:44 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 23:34:44 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 23:34:44 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 23:34:44 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 23:34:44 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:34:44 - src.pipeline - INFO - Starting background tasks...
2026-02-07 23:34:44 - src.pipeline - INFO - Reflection loop started
2026-02-07 23:34:44 - src.pipeline - INFO - Decay manager started
2026-02-07 23:34:44 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 23:34:44 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:34:44 - src.terminal_ui - INFO - New conversation started with session_id: e4cecaea-c813-4e30-bc71-af1c29437fc8
2026-02-07 23:34:44 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 23:34:44 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 23:34:44 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 23:34:53 - src.pipeline - INFO - Processing turn 1: my name is king kong...
2026-02-07 23:34:57 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.00
2026-02-07 23:34:59 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1641.79ms: 2 memories, 112 tokens
2026-02-07 23:35:04 - src.pipeline - INFO - Response generated: Welcome, King Kong. Is there something I can assis...
2026-02-07 23:35:04 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 23:35:04 - src.pipeline - INFO - PINNED memory stored in category 'identity': my name is king kong...
2026-02-07 23:35:15 - src.pipeline - INFO - Processing turn 2: Your name is Jarvis...
2026-02-07 23:35:19 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:35:21 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1819.02ms: 4 memories, 177 tokens
2026-02-07 23:35:25 - src.pipeline - INFO - Response generated: That's not correct, King Kong. I don't have a pers...
2026-02-07 23:35:25 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-07 23:35:25 - src.pipeline - INFO - PINNED memory stored in category 'assistant_identity': Your name is Jarvis...
2026-02-07 23:35:36 - src.pipeline - INFO - Processing turn 3: I live in Paris...
2026-02-07 23:35:40 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:35:43 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2250.52ms: 2 memories, 124 tokens
2026-02-07 23:35:46 - src.pipeline - INFO - Response generated: Paris is a beautiful city, King Kong. What brings ...
2026-02-07 23:35:46 - src.pipeline - INFO - Turn 3 completed successfully
2026-02-07 23:35:46 - src.pipeline - INFO - PINNED memory stored in category 'user_context': I live in Paris...
2026-02-07 23:36:06 - src.pipeline - INFO - Processing turn 4: My wife is Sarah...
2026-02-07 23:36:48 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:36:51 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2904.31ms: 2 memories, 129 tokens
2026-02-07 23:36:54 - src.pipeline - INFO - Response generated: It can be challenging to balance personal and publ...
2026-02-07 23:36:54 - src.pipeline - INFO - Turn 4 completed successfully
2026-02-07 23:36:54 - src.pipeline - INFO - PINNED memory stored in category 'relationships': My wife is Sarah...
2026-02-07 23:39:47 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-07 23:39:47 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-07 23:39:47 - src.pipeline - INFO - Stopping background tasks...
2026-02-07 23:39:47 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-07 23:39:47 - src.decay_manager - INFO - Decay Manager stopped
2026-02-07 23:39:47 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-07 23:39:47 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-07 23:39:47 - src.pipeline - INFO - All background tasks stopped
2026-02-07 23:39:48 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-07 23:39:48 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-07 23:39:48 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-07 23:39:48 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-07 23:51:46 - __main__ - INFO - Logging initialized at level: INFO
2026-02-07 23:51:46 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-07 23:51:46 - __main__ - INFO - Storage directory ready: data
2026-02-07 23:51:46 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-07 23:51:46 - __main__ - INFO - Storage directory ready: data\archive
2026-02-07 23:51:46 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-07 23:51:46 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-07 23:51:46 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-07 23:51:47 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-07 23:51:47 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-07 23:51:47 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-07 23:51:48 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-07 23:51:49 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-07 23:51:49 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-07 23:51:49 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-07 23:51:50 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-07 23:51:50 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-07 23:51:50 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-07 23:51:50 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-07 23:51:50 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:51:50 - src.pipeline - INFO - Starting background tasks...
2026-02-07 23:51:50 - src.pipeline - INFO - Reflection loop started
2026-02-07 23:51:50 - src.pipeline - INFO - Decay manager started
2026-02-07 23:51:50 - src.pipeline - INFO - All background tasks started successfully
2026-02-07 23:51:50 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-07 23:51:50 - src.terminal_ui - INFO - New conversation started with session_id: bcd48b19-d34f-4dd0-b417-ff4529ade79c
2026-02-07 23:51:50 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-07 23:51:50 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-07 23:51:50 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-07 23:52:02 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-07 23:52:06 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.00
2026-02-07 23:52:08 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1698.73ms: 10 memories, 340 tokens
2026-02-07 23:52:12 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-07 23:52:12 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-07 23:52:12 - src.pipeline - INFO - PINNED memory stored in category 'preferences': hi...
2026-02-07 23:52:19 - src.pipeline - INFO - Processing turn 2: I love Python...
2026-02-07 23:52:24 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-07 23:52:25 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1147.53ms: 10 memories, 341 tokens
2026-02-07 23:52:28 - src.pipeline - INFO - Response generated: Python is a popular and versatile language. What a...
2026-02-07 23:52:28 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-07 23:52:28 - src.pipeline - INFO - PINNED memory stored in category 'preferences': I love Python...
2026-02-08 00:00:59 - src.pipeline - INFO - Processing turn 3: I code in python...
2026-02-08 00:01:05 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-08 00:01:06 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1306.19ms: 10 memories, 345 tokens
2026-02-08 00:01:14 - src.pipeline - INFO - Response generated: That's great. Python has a vast number of librarie...
2026-02-08 00:01:14 - src.pipeline - INFO - Turn 3 completed successfully
2026-02-08 00:01:14 - src.pipeline - INFO - PINNED memory stored in category 'preferences': I code in python...
2026-02-08 00:03:06 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-08 00:03:06 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-08 00:03:06 - src.pipeline - INFO - Stopping background tasks...
2026-02-08 00:03:06 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-08 00:03:06 - src.decay_manager - INFO - Decay Manager stopped
2026-02-08 00:03:06 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-08 00:03:06 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-08 00:03:06 - src.pipeline - INFO - All background tasks stopped
2026-02-08 00:03:06 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-08 00:03:06 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-08 00:03:06 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-08 00:03:06 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-08 00:25:42 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-08 00:25:44 - src.decay_manager - INFO - Decay cycle complete: processed 5 memories, archived 0
2026-02-08 00:27:48 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-08 00:27:51 - src.decay_manager - INFO - Decay cycle complete: processed 5 memories, archived 0
2026-02-08 01:00:26 - __main__ - INFO - Logging initialized at level: INFO
2026-02-08 01:00:26 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-08 01:00:26 - __main__ - INFO - Storage directory ready: data
2026-02-08 01:00:26 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-08 01:00:26 - __main__ - INFO - Storage directory ready: data\archive
2026-02-08 01:00:26 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 01:00:26 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-08 01:00:26 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-08 01:00:27 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-08 01:00:27 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-08 01:00:28 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-08 01:00:29 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-08 01:00:29 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-08 01:00:29 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-08 01:00:29 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-08 01:00:31 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 1/3): Unable to allocate 250. MiB for an array with shape (512, 128256) and data type float32
2026-02-08 01:00:31 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 2/3)
2026-02-08 01:00:32 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-08 01:00:32 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-08 01:00:32 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-08 01:00:32 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-08 01:00:32 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-08 01:00:32 - src.pipeline - INFO - Starting background tasks...
2026-02-08 01:00:32 - src.pipeline - INFO - Reflection loop started
2026-02-08 01:00:32 - src.pipeline - INFO - Decay manager started
2026-02-08 01:00:32 - src.pipeline - INFO - All background tasks started successfully
2026-02-08 01:00:32 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-08 01:00:32 - src.terminal_ui - INFO - New conversation started with session_id: 8576427f-b9bb-418a-bdae-909e7dc20099
2026-02-08 01:00:32 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-08 01:00:32 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-08 01:00:32 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-08 01:02:37 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-08 01:02:43 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.50
2026-02-08 01:02:46 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2049.26ms: 10 memories, 350 tokens
2026-02-08 01:02:55 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-08 01:02:55 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-08 01:02:55 - src.pipeline - INFO - PINNED memory stored in category 'preferences': hi...
2026-02-08 01:05:12 - src.pipeline - INFO - Processing turn 2: The bakery sells delicious cakes...
2026-02-08 01:05:20 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-08 01:05:23 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2198.94ms: 10 memories, 352 tokens
2026-02-08 01:05:27 - src.pipeline - INFO - Response generated: It sounds like you're thinking about getting somet...
2026-02-08 01:05:27 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-08 01:05:27 - src.pipeline - INFO - PINNED memory stored in category 'preferences': The bakery sells delicious cakes...
2026-02-08 01:05:57 - src.pipeline - INFO - Processing turn 3: The store sells shoes...
2026-02-08 01:06:05 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.00
2026-02-08 01:06:07 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1888.82ms: 10 memories, 358 tokens
2026-02-08 01:06:14 - src.pipeline - INFO - Response generated: It seems like you're confusing it with a different...
2026-02-08 01:06:14 - src.pipeline - INFO - Turn 3 completed successfully
2026-02-08 01:06:14 - src.pipeline - INFO - PINNED memory stored in category 'preferences': The store sells shoes...
2026-02-08 01:12:33 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-08 01:12:33 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-08 01:12:33 - src.pipeline - INFO - Stopping background tasks...
2026-02-08 01:12:33 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-08 01:12:33 - src.decay_manager - INFO - Decay Manager stopped
2026-02-08 01:12:33 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-08 01:12:33 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-08 01:12:33 - src.pipeline - INFO - All background tasks stopped
2026-02-08 01:12:33 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-08 01:12:33 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-08 01:12:33 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-08 01:12:34 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-08 01:25:44 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-08 01:25:47 - src.decay_manager - INFO - Decay cycle complete: processed 5 memories, archived 0
2026-02-08 01:27:51 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-08 01:27:53 - src.decay_manager - INFO - Decay cycle complete: processed 5 memories, archived 0
2026-02-08 01:33:09 - __main__ - INFO - Logging initialized at level: INFO
2026-02-08 01:33:09 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-08 01:33:09 - __main__ - INFO - Storage directory ready: data
2026-02-08 01:33:09 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-08 01:33:09 - __main__ - INFO - Storage directory ready: data\archive
2026-02-08 01:33:09 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 01:33:09 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-08 01:33:09 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-08 01:33:09 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-08 01:33:09 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-08 01:33:10 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-08 01:33:12 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-08 01:33:12 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-08 01:33:12 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-08 01:33:12 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-08 01:33:14 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-08 01:33:14 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-08 01:33:14 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-08 01:33:14 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-08 01:33:14 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-08 01:33:14 - src.pipeline - INFO - Starting background tasks...
2026-02-08 01:33:14 - src.pipeline - INFO - Reflection loop started
2026-02-08 01:33:14 - src.pipeline - INFO - Decay manager started
2026-02-08 01:33:14 - src.pipeline - INFO - All background tasks started successfully
2026-02-08 01:33:14 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-08 01:33:14 - src.terminal_ui - INFO - New conversation started with session_id: 5cead155-283f-4ada-97d2-3d056dfea1e2
2026-02-08 01:33:14 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-08 01:33:14 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-08 01:33:14 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-08 01:33:30 - src.pipeline - INFO - Processing turn 1: The sky is blue...
2026-02-08 01:33:39 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-08 01:33:42 - src.retrieval_gatekeeper - INFO - Retrieval completed in 3180.51ms: 0 memories, 98 tokens
2026-02-08 01:33:56 - src.pipeline - INFO - Response generated: The color of the sky can vary depending on the tim...
2026-02-08 01:33:56 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-08 01:33:56 - src.pipeline - INFO - PINNED memory stored in category 'preferences': The sky is blue...
2026-02-08 01:34:01 - src.pipeline - INFO - Processing turn 2: The sky is blue...
2026-02-08 01:34:08 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.00
2026-02-08 01:34:10 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2223.70ms: 0 memories, 104 tokens
2026-02-08 01:34:15 - src.pipeline - INFO - Response generated: That's correct. The color of the sky can indeed ap...
2026-02-08 01:34:15 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-08 01:34:15 - src.pipeline - INFO - PINNED memory stored in category 'preferences': The sky is blue...
2026-02-08 01:35:22 - __main__ - INFO - Logging initialized at level: INFO
2026-02-08 01:35:22 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-08 01:35:22 - __main__ - INFO - Storage directory ready: data
2026-02-08 01:35:22 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-08 01:35:22 - __main__ - INFO - Storage directory ready: data\archive
2026-02-08 01:35:22 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 01:35:22 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-08 01:35:22 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-08 01:35:23 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-08 01:35:23 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-08 01:35:23 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-08 01:35:25 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-08 01:35:25 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-08 01:35:25 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-08 01:35:25 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-08 01:35:27 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-08 01:35:27 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-08 01:35:27 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-08 01:35:27 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-08 01:35:27 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-08 01:35:27 - src.pipeline - INFO - Starting background tasks...
2026-02-08 01:35:27 - src.pipeline - INFO - Reflection loop started
2026-02-08 01:35:27 - src.pipeline - INFO - Decay manager started
2026-02-08 01:35:27 - src.pipeline - INFO - All background tasks started successfully
2026-02-08 01:35:27 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-08 01:35:27 - src.terminal_ui - INFO - New conversation started with session_id: f3d2b836-0fa0-4467-8407-15fd50ec9665
2026-02-08 01:35:27 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-08 01:35:27 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-08 01:35:27 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-08 01:35:42 - src.pipeline - INFO - Processing turn 1: The sky is blue...
2026-02-08 01:35:51 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-08 01:35:53 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2474.37ms: 0 memories, 109 tokens
2026-02-08 01:36:05 - src.pipeline - INFO - Response generated: The color of the sky can vary depending on the tim...
2026-02-08 01:36:05 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-08 01:36:05 - src.pipeline - INFO - PINNED memory stored in category 'preferences': The sky is blue...
2026-02-08 01:36:07 - src.pipeline - INFO - Processing turn 2: The sky is blue...
2026-02-08 01:36:14 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.95
2026-02-08 01:36:17 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2353.18ms: 0 memories, 114 tokens
2026-02-08 01:36:22 - src.pipeline - INFO - Response generated: That's correct. The color of the sky can indeed ap...
2026-02-08 01:36:22 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-08 01:36:22 - src.pipeline - INFO - PINNED memory stored in category 'preferences': The sky is blue...
2026-02-08 01:46:58 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-08 01:46:58 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-08 01:46:58 - src.pipeline - INFO - Stopping background tasks...
2026-02-08 01:46:58 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-08 01:46:58 - src.decay_manager - INFO - Decay Manager stopped
2026-02-08 01:46:58 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-08 01:46:58 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-08 01:46:58 - src.pipeline - INFO - All background tasks stopped
2026-02-08 01:46:58 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-08 01:46:58 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-08 01:46:58 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-08 01:46:59 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-08 02:10:32 - __main__ - INFO - Logging initialized at level: INFO
2026-02-08 02:10:32 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-08 02:10:32 - __main__ - INFO - Storage directory ready: data
2026-02-08 02:10:32 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-08 02:10:32 - __main__ - INFO - Storage directory ready: data\archive
2026-02-08 02:10:32 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 02:10:32 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-08 02:10:32 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-08 02:10:32 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-08 02:10:32 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-08 02:10:33 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-08 02:10:35 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-08 02:10:35 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-08 02:10:35 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-08 02:10:35 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-08 02:10:37 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-08 02:10:37 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-08 02:10:37 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-08 02:10:37 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-08 02:10:37 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-08 02:10:37 - src.pipeline - INFO - Starting background tasks...
2026-02-08 02:10:37 - src.pipeline - INFO - Reflection loop started
2026-02-08 02:10:37 - src.pipeline - INFO - Decay manager started
2026-02-08 02:10:37 - src.pipeline - INFO - All background tasks started successfully
2026-02-08 02:10:37 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-08 02:10:37 - src.terminal_ui - INFO - New conversation started with session_id: af2fde16-f31f-44b7-ae9d-3927a8c3c875
2026-02-08 02:10:37 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-08 02:10:37 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-08 02:10:37 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-08 02:11:46 - __main__ - INFO - Logging initialized at level: INFO
2026-02-08 02:11:46 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-08 02:11:46 - __main__ - INFO - Storage directory ready: data
2026-02-08 02:11:46 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-08 02:11:46 - __main__ - INFO - Storage directory ready: data\archive
2026-02-08 02:11:46 - __main__ - INFO - Path validation complete
2026-02-08 02:11:46 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 02:11:46 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 02:11:46 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-08 02:11:46 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-08 02:11:47 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-08 02:11:47 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-08 02:11:48 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-08 02:11:49 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-08 02:11:50 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-08 02:11:50 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-08 02:11:50 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-08 02:11:52 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-08 02:11:52 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-08 02:11:52 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-08 02:11:52 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-08 02:11:52 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-08 02:11:52 - src.pipeline - INFO - Starting background tasks...
2026-02-08 02:11:52 - src.pipeline - INFO - Reflection loop started
2026-02-08 02:11:52 - src.pipeline - INFO - Decay manager started
2026-02-08 02:11:52 - src.pipeline - INFO - All background tasks started successfully
2026-02-08 02:11:52 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-08 02:11:52 - __main__ - INFO - Pipeline initialized successfully
2026-02-08 02:11:52 - src.terminal_ui - INFO - New conversation started with session_id: c89bf501-3201-4a40-ae34-3442b8231bbc
2026-02-08 02:11:52 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-08 02:11:52 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-08 02:11:52 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-08 02:20:53 - __main__ - INFO - Logging initialized at level: INFO
2026-02-08 02:20:53 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-08 02:20:53 - __main__ - INFO - Storage directory ready: data
2026-02-08 02:20:53 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-08 02:20:53 - __main__ - INFO - Storage directory ready: data\archive
2026-02-08 02:20:53 - __main__ - INFO - Path validation complete
2026-02-08 02:20:53 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 02:20:53 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 02:20:53 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-08 02:20:53 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-08 02:20:53 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-08 02:20:53 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-08 02:20:55 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-08 02:20:56 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-08 02:20:56 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-08 02:20:56 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-08 02:20:56 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-08 02:20:58 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-08 02:20:58 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-08 02:20:58 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-08 02:20:58 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-08 02:20:58 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-08 02:20:58 - src.pipeline - INFO - Starting background tasks...
2026-02-08 02:20:58 - src.pipeline - INFO - Reflection loop started
2026-02-08 02:20:58 - src.pipeline - INFO - Decay manager started
2026-02-08 02:20:58 - src.pipeline - INFO - All background tasks started successfully
2026-02-08 02:20:58 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-08 02:20:58 - __main__ - INFO - Pipeline initialized successfully
2026-02-08 02:20:58 - src.terminal_ui - INFO - New conversation started with session_id: 799e4b74-d516-4629-a317-66cef0955183
2026-02-08 02:20:58 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-08 02:20:58 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-08 02:20:58 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-08 02:21:21 - src.graph_engine - INFO - Graph visualization saved to D:\Neurohacks\memory_graph.html
2026-02-08 03:10:37 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-08 03:10:39 - src.decay_manager - INFO - Decay cycle complete: processed 0 memories, archived 0
2026-02-08 03:11:52 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-08 03:11:53 - src.decay_manager - INFO - Decay cycle complete: processed 0 memories, archived 0
2026-02-08 03:12:42 - __main__ - INFO - Logging initialized at level: INFO
2026-02-08 03:12:42 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-08 03:12:42 - __main__ - INFO - Storage directory ready: data
2026-02-08 03:12:42 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-08 03:12:42 - __main__ - INFO - Storage directory ready: data\archive
2026-02-08 03:12:42 - __main__ - INFO - Path validation complete
2026-02-08 03:12:42 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 03:12:42 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 03:12:42 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-08 03:12:42 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-08 03:12:43 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-08 03:12:43 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-08 03:12:44 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-08 03:12:45 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-08 03:12:46 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-08 03:12:46 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-08 03:12:46 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-08 03:12:47 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 1/3): Failed to create llama_context
2026-02-08 03:12:47 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 2/3)
2026-02-08 03:12:48 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 2/3): Failed to create llama_context
2026-02-08 03:12:48 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 3/3)
2026-02-08 03:12:49 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 3/3): Failed to create llama_context
2026-02-08 03:12:49 - __main__ - ERROR - Failed to initialize pipeline: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 129, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-08 03:12:49 - __main__ - ERROR - Unexpected error in main loop: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 208, in async_main
    pipeline = await initialize_pipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\main.py", line 129, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-08 03:12:49 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-08 03:12:49 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-08 03:12:49 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-08 03:12:59 - __main__ - INFO - Logging initialized at level: INFO
2026-02-08 03:12:59 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-08 03:12:59 - __main__ - INFO - Storage directory ready: data
2026-02-08 03:12:59 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-08 03:12:59 - __main__ - INFO - Storage directory ready: data\archive
2026-02-08 03:12:59 - __main__ - INFO - Path validation complete
2026-02-08 03:12:59 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 03:12:59 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 03:12:59 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-08 03:12:59 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-08 03:12:59 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-08 03:12:59 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-08 03:13:00 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-08 03:13:04 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-08 03:13:04 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-08 03:13:04 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-08 03:13:04 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-08 03:13:05 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 1/3): Failed to create llama_context
2026-02-08 03:13:05 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 2/3)
2026-02-08 03:13:06 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 2/3): Failed to create llama_context
2026-02-08 03:13:06 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 3/3)
2026-02-08 03:13:07 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 3/3): Failed to create llama_context
2026-02-08 03:13:07 - __main__ - ERROR - Failed to initialize pipeline: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 129, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-08 03:13:07 - __main__ - ERROR - Unexpected error in main loop: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 208, in async_main
    pipeline = await initialize_pipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\main.py", line 129, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-08 03:13:07 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-08 03:13:07 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-08 03:13:07 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-08 03:14:14 - __main__ - INFO - Logging initialized at level: INFO
2026-02-08 03:14:14 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-08 03:14:14 - __main__ - INFO - Storage directory ready: data
2026-02-08 03:14:14 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-08 03:14:14 - __main__ - INFO - Storage directory ready: data\archive
2026-02-08 03:14:14 - __main__ - INFO - Path validation complete
2026-02-08 03:14:14 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 03:14:14 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-08 03:14:14 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-08 03:14:14 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-08 03:14:15 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-08 03:14:15 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-08 03:14:16 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-08 03:14:17 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-08 03:14:18 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-08 03:14:18 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-08 03:14:18 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-08 03:14:19 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-08 03:14:19 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-08 03:14:19 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-08 03:14:19 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-08 03:14:19 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-08 03:14:19 - src.pipeline - INFO - Starting background tasks...
2026-02-08 03:14:19 - src.pipeline - INFO - Reflection loop started
2026-02-08 03:14:19 - src.pipeline - INFO - Decay manager started
2026-02-08 03:14:19 - src.pipeline - INFO - All background tasks started successfully
2026-02-08 03:14:19 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-08 03:14:19 - __main__ - INFO - Pipeline initialized successfully
2026-02-08 03:14:19 - src.terminal_ui - INFO - New conversation started with session_id: 658bf04f-3d07-4157-80c2-6ba7723ec305
2026-02-08 03:14:19 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-08 03:14:19 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-08 03:14:19 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-08 03:14:26 - src.graph_engine - INFO - Graph visualization saved to D:\Neurohacks\memory_graph.html
2026-02-08 03:15:20 - src.graph_engine - INFO - Graph visualization saved to D:\Neurohacks\memory_graph.html
2026-02-08 03:19:07 - src.graph_engine - INFO - Graph visualization saved to D:\Neurohacks\memory_graph.html
2026-02-08 03:19:17 - src.graph_engine - INFO - Graph visualization saved to D:\Neurohacks\memory_graph.html
2026-02-08 03:19:24 - src.graph_engine - INFO - Graph visualization saved to D:\Neurohacks\memory_graph.html
2026-02-08 03:19:32 - src.graph_engine - INFO - Graph visualization saved to D:\Neurohacks\memory_graph.html
2026-02-08 03:19:40 - src.graph_engine - INFO - Graph visualization saved to D:\Neurohacks\memory_graph.html
2026-02-08 03:19:47 - src.graph_engine - INFO - Graph visualization saved to D:\Neurohacks\memory_graph.html
2026-02-09 11:20:56 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 11:20:56 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 11:20:56 - __main__ - INFO - Storage directory ready: data
2026-02-09 11:20:56 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 11:20:56 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 11:20:56 - __main__ - INFO - Path validation complete
2026-02-09 11:20:56 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 11:20:56 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 11:20:56 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 11:20:56 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 11:20:56 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 11:20:56 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-09 11:20:56 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf with LoRA adapter ./data/models/fine_tuned_lora.gguf
2026-02-09 11:20:58 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 11:21:01 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 11:21:02 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 11:21:02 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 11:21:02 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 11:21:03 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-09 11:21:03 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-09 11:21:03 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-09 11:21:03 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-09 11:21:03 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-09 11:21:03 - src.pipeline - INFO - Starting background tasks...
2026-02-09 11:21:03 - src.pipeline - INFO - Reflection loop started
2026-02-09 11:21:03 - src.pipeline - INFO - Decay manager started
2026-02-09 11:21:03 - src.pipeline - INFO - All background tasks started successfully
2026-02-09 11:21:03 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-09 11:21:03 - __main__ - INFO - Pipeline initialized successfully
2026-02-09 11:21:03 - src.terminal_ui - INFO - New conversation started with session_id: 9f3e4668-5a45-4bbc-8ac2-2dee7ebb099a
2026-02-09 11:21:03 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-09 11:21:03 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-09 11:21:03 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-09 11:21:13 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-09 11:21:18 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:21:19 - src.retrieval_gatekeeper - INFO - Retrieval completed in 983.92ms: 0 memories, 119 tokens
2026-02-09 11:21:19 - src.terminal_ui - ERROR - Error processing message: 'MemoryExtraction' object has no attribute 'content'
Traceback (most recent call last):
  File "D:\Neurohacks\src\terminal_ui.py", line 123, in _process_message
    response = await self.pipeline.process_turn(message.strip(), self.context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 219, in process_turn
    logger.debug(f"Memory skipped: Category={extraction.category.value}, Content='{extraction.content}'")
                                                                                   ^^^^^^^^^^^^^^^^^^
AttributeError: 'MemoryExtraction' object has no attribute 'content'
2026-02-09 11:21:24 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-09 11:21:28 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:21:29 - src.retrieval_gatekeeper - INFO - Retrieval completed in 948.19ms: 0 memories, 119 tokens
2026-02-09 11:21:29 - src.terminal_ui - ERROR - Error processing message: 'MemoryExtraction' object has no attribute 'content'
Traceback (most recent call last):
  File "D:\Neurohacks\src\terminal_ui.py", line 123, in _process_message
    response = await self.pipeline.process_turn(message.strip(), self.context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 219, in process_turn
    logger.debug(f"Memory skipped: Category={extraction.category.value}, Content='{extraction.content}'")
                                                                                   ^^^^^^^^^^^^^^^^^^
AttributeError: 'MemoryExtraction' object has no attribute 'content'
2026-02-09 11:21:33 - src.pipeline - INFO - Processing turn 1: hello...
2026-02-09 11:21:37 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:21:38 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1000.20ms: 0 memories, 119 tokens
2026-02-09 11:21:38 - src.terminal_ui - ERROR - Error processing message: 'MemoryExtraction' object has no attribute 'content'
Traceback (most recent call last):
  File "D:\Neurohacks\src\terminal_ui.py", line 123, in _process_message
    response = await self.pipeline.process_turn(message.strip(), self.context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 219, in process_turn
    logger.debug(f"Memory skipped: Category={extraction.category.value}, Content='{extraction.content}'")
                                                                                   ^^^^^^^^^^^^^^^^^^
AttributeError: 'MemoryExtraction' object has no attribute 'content'
2026-02-09 11:21:48 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-09 11:21:52 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:21:53 - src.retrieval_gatekeeper - INFO - Retrieval completed in 994.65ms: 0 memories, 119 tokens
2026-02-09 11:21:53 - src.terminal_ui - ERROR - Error processing message: 'MemoryExtraction' object has no attribute 'content'
Traceback (most recent call last):
  File "D:\Neurohacks\src\terminal_ui.py", line 123, in _process_message
    response = await self.pipeline.process_turn(message.strip(), self.context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 219, in process_turn
    logger.debug(f"Memory skipped: Category={extraction.category.value}, Content='{extraction.content}'")
                                                                                   ^^^^^^^^^^^^^^^^^^
AttributeError: 'MemoryExtraction' object has no attribute 'content'
2026-02-09 11:21:54 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-09 11:21:58 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:21:59 - src.retrieval_gatekeeper - INFO - Retrieval completed in 948.41ms: 0 memories, 119 tokens
2026-02-09 11:21:59 - src.terminal_ui - ERROR - Error processing message: 'MemoryExtraction' object has no attribute 'content'
Traceback (most recent call last):
  File "D:\Neurohacks\src\terminal_ui.py", line 123, in _process_message
    response = await self.pipeline.process_turn(message.strip(), self.context)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 219, in process_turn
    logger.debug(f"Memory skipped: Category={extraction.category.value}, Content='{extraction.content}'")
                                                                                   ^^^^^^^^^^^^^^^^^^
AttributeError: 'MemoryExtraction' object has no attribute 'content'
2026-02-09 11:23:25 - __main__ - INFO - Shutting down...
2026-02-09 11:23:25 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-09 11:23:25 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-09 11:23:25 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-09 11:23:25 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-09 11:23:25 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-09 11:23:25 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-09 11:23:25 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-09 11:23:51 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 11:23:51 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 11:23:51 - __main__ - INFO - Storage directory ready: data
2026-02-09 11:23:51 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 11:23:51 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 11:23:51 - __main__ - INFO - Path validation complete
2026-02-09 11:23:51 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 11:23:51 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 11:23:51 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 11:23:51 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 11:23:52 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 11:23:52 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-09 11:23:52 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf with LoRA adapter ./data/models/fine_tuned_lora.gguf
2026-02-09 11:23:55 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 11:23:56 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 11:23:57 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 11:23:57 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 11:23:57 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 11:23:59 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-09 11:23:59 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-09 11:23:59 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-09 11:23:59 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-09 11:23:59 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-09 11:23:59 - src.pipeline - INFO - Starting background tasks...
2026-02-09 11:23:59 - src.pipeline - INFO - Reflection loop started
2026-02-09 11:23:59 - src.pipeline - INFO - Decay manager started
2026-02-09 11:23:59 - src.pipeline - INFO - All background tasks started successfully
2026-02-09 11:23:59 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-09 11:23:59 - __main__ - INFO - Pipeline initialized successfully
2026-02-09 11:23:59 - src.terminal_ui - INFO - New conversation started with session_id: 813efb9a-9b68-4f7b-81d8-0cdf7d33acf1
2026-02-09 11:23:59 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-09 11:23:59 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-09 11:23:59 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-09 11:24:01 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-09 11:24:07 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:24:08 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1225.00ms: 0 memories, 119 tokens
2026-02-09 11:24:14 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-09 11:24:14 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-09 11:24:31 - src.pipeline - INFO - Processing turn 2: my name is kratos...
2026-02-09 11:24:38 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:24:39 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1184.73ms: 0 memories, 119 tokens
2026-02-09 11:24:42 - src.pipeline - INFO - Response generated: Kratos. What's on your mind? Need help with someth...
2026-02-09 11:24:42 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-09 11:26:51 - __main__ - INFO - Shutting down...
2026-02-09 11:26:51 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-09 11:26:51 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-09 11:26:51 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-09 11:26:51 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-09 11:26:51 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-09 11:26:51 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-09 11:26:52 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-09 11:26:58 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 11:26:58 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 11:26:58 - __main__ - INFO - Storage directory ready: data
2026-02-09 11:26:58 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 11:26:58 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 11:26:58 - __main__ - INFO - Path validation complete
2026-02-09 11:26:58 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 11:26:58 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 11:26:58 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 11:26:58 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 11:26:59 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 11:26:59 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-09 11:26:59 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf with LoRA adapter ./data/models/fine_tuned_lora.gguf
2026-02-09 11:27:01 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 11:27:02 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 11:27:03 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 11:27:03 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 11:27:03 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 11:27:05 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-09 11:27:05 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-09 11:27:05 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-09 11:27:05 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-09 11:27:05 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-09 11:27:05 - src.pipeline - INFO - Starting background tasks...
2026-02-09 11:27:05 - src.pipeline - INFO - Reflection loop started
2026-02-09 11:27:05 - src.pipeline - INFO - Decay manager started
2026-02-09 11:27:05 - src.pipeline - INFO - All background tasks started successfully
2026-02-09 11:27:05 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-09 11:27:05 - __main__ - INFO - Pipeline initialized successfully
2026-02-09 11:27:05 - src.terminal_ui - INFO - New conversation started with session_id: 5d8f3094-293c-46da-b84e-aff60c5e6b8f
2026-02-09 11:27:05 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-09 11:27:05 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-09 11:27:05 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-09 11:27:12 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:27:12 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-09 11:27:12 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:27:14 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1201.58ms: 0 memories, 119 tokens
2026-02-09 11:27:18 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-09 11:27:18 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-09 11:27:42 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:27:42 - src.pipeline - INFO - Processing turn 2: my name is humpty dumpty...
2026-02-09 11:27:43 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:27:44 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1280.11ms: 0 memories, 119 tokens
2026-02-09 11:27:48 - src.pipeline - INFO - Response generated: Nice to meet you, Humpty Dumpty. Is there somethin...
2026-02-09 11:27:48 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-09 11:28:09 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:28:09 - src.pipeline - INFO - Processing turn 3: my name is humpy always remember that...
2026-02-09 11:28:09 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:28:11 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2140.28ms: 0 memories, 119 tokens
2026-02-09 11:28:17 - src.pipeline - INFO - Response generated: I'll make sure to remember that, Humpy. I'll keep ...
2026-02-09 11:28:17 - src.pipeline - INFO - Turn 3 completed successfully
2026-02-09 11:28:32 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:28:32 - src.pipeline - INFO - Processing turn 4: set an alarm for 11am...
2026-02-09 11:28:32 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.50
2026-02-09 11:28:35 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2775.07ms: 0 memories, 119 tokens
2026-02-09 11:28:42 - src.pipeline - INFO - Response generated: I've taken note that you'd like to be reminded abo...
2026-02-09 11:28:42 - src.pipeline - INFO - Turn 4 completed successfully
2026-02-09 11:31:05 - __main__ - INFO - Shutting down...
2026-02-09 11:31:05 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-09 11:31:05 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-09 11:31:05 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-09 11:31:06 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-09 11:31:06 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-09 11:31:06 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-09 11:31:06 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-09 11:31:13 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 11:31:13 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 11:31:13 - __main__ - INFO - Storage directory ready: data
2026-02-09 11:31:13 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 11:31:13 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 11:31:13 - __main__ - INFO - Path validation complete
2026-02-09 11:31:13 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 11:31:13 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 11:31:13 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 11:31:13 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 11:31:13 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 11:31:13 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-09 11:31:13 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf with LoRA adapter ./data/models/fine_tuned_lora.gguf
2026-02-09 11:31:15 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 11:31:17 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 11:31:17 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 11:31:17 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 11:31:17 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 11:31:19 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-09 11:31:19 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-09 11:31:19 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-09 11:31:19 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-09 11:31:19 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-09 11:31:19 - src.pipeline - INFO - Starting background tasks...
2026-02-09 11:31:19 - src.pipeline - INFO - Reflection loop started
2026-02-09 11:31:19 - src.pipeline - INFO - Decay manager started
2026-02-09 11:31:19 - src.pipeline - INFO - All background tasks started successfully
2026-02-09 11:31:19 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-09 11:31:19 - __main__ - INFO - Pipeline initialized successfully
2026-02-09 11:31:19 - src.terminal_ui - INFO - New conversation started with session_id: a1d9c8fe-44c3-4bdf-8433-ce123251a1ba
2026-02-09 11:31:19 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-09 11:31:19 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-09 11:31:19 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-09 11:31:27 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.85
2026-02-09 11:31:27 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-09 11:31:28 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.85
2026-02-09 11:31:29 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1245.34ms: 0 memories, 119 tokens
2026-02-09 11:31:34 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-09 11:31:34 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-09 11:31:34 - src.pipeline - INFO - PINNED memory stored in category 'preferences': hi...
2026-02-09 11:31:43 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.85
2026-02-09 11:31:43 - src.pipeline - INFO - Processing turn 2: my name is ishaan...
2026-02-09 11:31:44 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.85
2026-02-09 11:31:46 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2654.10ms: 2 memories, 179 tokens
2026-02-09 11:31:48 - src.pipeline - INFO - Response generated: Hello Ishaan. What's on your mind?...
2026-02-09 11:31:48 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-09 11:31:48 - src.pipeline - INFO - PINNED memory stored in category 'identity': my name is ishaan...
2026-02-09 11:31:50 - __main__ - INFO - Shutting down...
2026-02-09 11:31:50 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-09 11:31:50 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-09 11:31:50 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-09 11:31:50 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-09 11:31:50 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-09 11:31:50 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-09 11:31:51 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-09 12:38:12 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 12:38:12 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 12:38:12 - __main__ - INFO - Storage directory ready: data
2026-02-09 12:38:12 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 12:38:12 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 12:38:12 - __main__ - INFO - Path validation complete
2026-02-09 12:38:12 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 12:38:12 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 12:38:12 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 12:38:12 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 12:38:12 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 12:38:12 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-09 12:38:12 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf with LoRA adapter ./data/models/fine_tuned_lora.gguf
2026-02-09 12:38:14 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 12:38:15 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 12:38:16 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 12:38:16 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 12:38:16 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 12:38:18 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-09 12:38:18 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-09 12:38:18 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-09 12:38:18 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-09 12:38:18 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-09 12:38:18 - src.pipeline - INFO - Starting background tasks...
2026-02-09 12:38:18 - src.pipeline - INFO - Reflection loop started
2026-02-09 12:38:18 - src.pipeline - INFO - Decay manager started
2026-02-09 12:38:18 - src.pipeline - INFO - All background tasks started successfully
2026-02-09 12:38:18 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-09 12:38:18 - __main__ - INFO - Pipeline initialized successfully
2026-02-09 12:38:18 - src.terminal_ui - INFO - New conversation started with session_id: 45949d1b-b6e0-47b7-8e03-c17a0d6c4868
2026-02-09 12:38:18 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-09 12:38:18 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-09 12:38:18 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-09 12:38:27 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.85
2026-02-09 12:38:27 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-09 12:38:27 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.85
2026-02-09 12:38:28 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1052.14ms: 0 memories, 126 tokens
2026-02-09 12:38:32 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-09 12:38:32 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-09 12:38:32 - src.pipeline - INFO - PINNED memory stored in category 'preferences': hi...
2026-02-09 12:41:06 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 12:41:06 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 12:41:06 - __main__ - INFO - Storage directory ready: data
2026-02-09 12:41:06 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 12:41:06 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 12:41:06 - __main__ - INFO - Path validation complete
2026-02-09 12:41:06 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 12:41:07 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 12:41:07 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 12:41:07 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 12:41:07 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 12:41:07 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf
2026-02-09 12:41:07 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf with LoRA adapter ./data/models/fine_tuned_lora.gguf
2026-02-09 12:41:09 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 12:41:10 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 12:41:11 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 12:41:11 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 12:41:11 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 12:41:12 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-09 12:41:12 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-09 12:41:12 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-09 12:41:12 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-09 12:41:12 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-09 12:41:12 - src.pipeline - INFO - Starting background tasks...
2026-02-09 12:41:12 - src.pipeline - INFO - Reflection loop started
2026-02-09 12:41:12 - src.pipeline - INFO - Decay manager started
2026-02-09 12:41:12 - src.pipeline - INFO - All background tasks started successfully
2026-02-09 12:41:12 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-09 12:41:12 - __main__ - INFO - Pipeline initialized successfully
2026-02-09 12:41:12 - src.terminal_ui - INFO - New conversation started with session_id: ed54ebfb-ea7e-4c51-a5a8-594183174e0f
2026-02-09 12:41:12 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-09 12:41:12 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-09 12:41:12 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-09 12:41:21 - src.memory_analyzer - INFO - SLM raw output: 'PINNED'
2026-02-09 12:41:21 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.85
2026-02-09 12:41:21 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-09 12:41:21 - src.memory_analyzer - INFO - SLM raw output: 'PINNED'
2026-02-09 12:41:21 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=0.85
2026-02-09 12:41:22 - src.retrieval_gatekeeper - INFO - Retrieval completed in 1071.14ms: 0 memories, 127 tokens
2026-02-09 12:41:26 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-09 12:41:26 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-09 12:41:26 - src.pipeline - INFO - PINNED memory stored in category 'preferences': hi...
2026-02-09 12:41:50 - __main__ - INFO - Shutting down...
2026-02-09 12:41:50 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-09 12:41:50 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-09 12:41:50 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-09 12:41:50 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-09 12:41:50 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-09 12:41:50 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-09 12:41:50 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-09 12:55:22 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 12:55:22 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 12:55:22 - __main__ - INFO - Storage directory ready: data
2026-02-09 12:55:22 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 12:55:22 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 12:55:22 - __main__ - INFO - Path validation complete
2026-02-09 12:55:22 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 12:55:22 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 12:55:22 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 12:55:22 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 12:55:22 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 12:55:22 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 12:55:22 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 12:55:24 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 12:55:25 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 12:55:26 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 12:55:26 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 12:55:26 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 12:55:27 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-09 12:55:27 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-09 12:55:27 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-09 12:55:27 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-09 12:55:27 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-09 12:55:27 - src.pipeline - INFO - Starting background tasks...
2026-02-09 12:55:27 - src.pipeline - INFO - Reflection loop started
2026-02-09 12:55:27 - src.pipeline - INFO - Decay manager started
2026-02-09 12:55:27 - src.pipeline - INFO - All background tasks started successfully
2026-02-09 12:55:27 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-09 12:55:27 - __main__ - INFO - Pipeline initialized successfully
2026-02-09 12:55:27 - src.terminal_ui - INFO - New conversation started with session_id: 5b3497ba-f87e-4605-82a1-73b27f727049
2026-02-09 12:55:27 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-09 12:55:27 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-09 12:55:27 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-09 12:55:48 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Simple greeting
Triples: [User, GREETING]'
2026-02-09 12:55:48 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-09 12:55:48 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-09 12:55:51 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Simple greeting
Triples: [User, GREETING]'
2026-02-09 12:55:51 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-09 12:55:54 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '4.', using pattern fallback
2026-02-09 12:55:54 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2736.37ms: 0 memories, 128 tokens
2026-02-09 12:55:57 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-09 12:55:57 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-09 12:56:28 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 1.0
Key info: Greeting
Triples: [User, Acknowledges, None]'
2026-02-09 12:56:28 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=1.00
2026-02-09 12:56:28 - src.pipeline - INFO - Processing turn 2: my name is KING KONG okay?...
2026-02-09 12:56:31 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Greeting
Triples: [User, GREETING, OKAY]'
2026-02-09 12:56:31 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-09 12:56:34 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '4', using pattern fallback
2026-02-09 12:56:35 - src.retrieval_gatekeeper - INFO - Retrieval completed in 4217.68ms: 2 memories, 187 tokens
2026-02-09 12:56:38 - src.pipeline - INFO - Response generated: Nice to meet you, King Kong. Is there something I ...
2026-02-09 12:56:38 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-09 12:57:14 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Simple greeting
Triples: []'
2026-02-09 12:57:14 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-09 12:57:14 - src.pipeline - INFO - Processing turn 3: Always remember my name is xoxo...
2026-02-09 12:57:17 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Greeting, no value
Triples: [User, Acknowledges, Messag'
2026-02-09 12:57:17 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-09 12:57:20 - src.retrieval_gatekeeper - INFO - Retrieval completed in 3208.02ms: 0 memories, 128 tokens
2026-02-09 12:57:23 - src.pipeline - INFO - Response generated: I'll keep that in mind, King Kong. Just to confirm...
2026-02-09 12:57:23 - src.pipeline - INFO - Turn 3 completed successfully
2026-02-09 13:00:53 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 13:00:53 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 13:00:53 - __main__ - INFO - Storage directory ready: data
2026-02-09 13:00:53 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 13:00:53 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 13:00:53 - __main__ - INFO - Path validation complete
2026-02-09 13:00:53 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 13:00:53 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 13:00:53 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 13:00:53 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 13:00:54 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 13:00:54 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 13:00:54 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 13:00:55 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 13:00:56 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 13:00:57 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 13:00:57 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 13:00:57 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 13:00:58 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-09 13:00:58 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-09 13:00:58 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-09 13:00:58 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-09 13:00:58 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-09 13:00:58 - src.pipeline - INFO - Starting background tasks...
2026-02-09 13:00:58 - src.pipeline - INFO - Reflection loop started
2026-02-09 13:00:58 - src.pipeline - INFO - Decay manager started
2026-02-09 13:00:58 - src.pipeline - INFO - All background tasks started successfully
2026-02-09 13:00:58 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-09 13:00:58 - __main__ - INFO - Pipeline initialized successfully
2026-02-09 13:00:58 - src.terminal_ui - INFO - New conversation started with session_id: e075b382-43a0-471f-8d74-7890c49b2651
2026-02-09 13:00:58 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-09 13:00:58 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-09 13:00:58 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-09 13:01:15 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Greeting
Triples: []'
2026-02-09 13:01:15 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-09 13:01:15 - src.pipeline - INFO - Processing turn 1: my name is king kong...
2026-02-09 13:01:17 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: 
Triples: []'
2026-02-09 13:01:17 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-09 13:01:20 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '4', using pattern fallback
2026-02-09 13:01:20 - src.retrieval_gatekeeper - INFO - Retrieval completed in 3559.48ms: 2 memories, 187 tokens
2026-02-09 13:01:25 - src.pipeline - INFO - Response generated: What can I assist you with, King Kong?...
2026-02-09 13:01:25 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-09 13:02:31 - __main__ - INFO - Shutting down...
2026-02-09 13:02:31 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-09 13:02:31 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-09 13:02:31 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-09 13:02:31 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-09 13:02:31 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-09 13:02:31 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-09 13:02:31 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-09 15:33:03 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 15:33:03 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 15:33:03 - __main__ - INFO - Storage directory ready: data
2026-02-09 15:33:03 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 15:33:03 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 15:33:03 - __main__ - INFO - Path validation complete
2026-02-09 15:33:03 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 15:33:03 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 15:33:03 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 15:33:03 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 15:33:04 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 15:33:04 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:33:04 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:33:05 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 15:33:06 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 15:33:07 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 15:33:07 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 15:33:07 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 15:33:08 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 1/3): Failed to create llama_context
2026-02-09 15:33:08 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 2/3)
2026-02-09 15:33:10 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 2/3): Failed to create llama_context
2026-02-09 15:33:10 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 3/3)
2026-02-09 15:33:11 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 3/3): Failed to create llama_context
2026-02-09 15:33:11 - __main__ - ERROR - Failed to initialize pipeline: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 129, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:33:11 - __main__ - ERROR - Unexpected error in main loop: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 208, in async_main
    pipeline = await initialize_pipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\main.py", line 129, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:33:11 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-09 15:33:11 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-09 15:33:11 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-09 15:33:26 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 15:33:26 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 15:33:26 - __main__ - INFO - Storage directory ready: data
2026-02-09 15:33:26 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 15:33:26 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 15:33:26 - __main__ - INFO - Path validation complete
2026-02-09 15:33:26 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 15:33:26 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 15:33:26 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 15:33:26 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 15:33:26 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 15:33:26 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:33:26 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:33:27 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 15:33:28 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 15:33:29 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 15:33:29 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 15:33:29 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 15:33:30 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 1/3): Failed to create llama_context
2026-02-09 15:33:30 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 2/3)
2026-02-09 15:33:31 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 2/3): Failed to create llama_context
2026-02-09 15:33:31 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 3/3)
2026-02-09 15:33:32 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 3/3): Failed to create llama_context
2026-02-09 15:33:32 - __main__ - ERROR - Failed to initialize pipeline: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 129, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:33:32 - __main__ - ERROR - Unexpected error in main loop: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 208, in async_main
    pipeline = await initialize_pipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\main.py", line 129, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:33:33 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-09 15:33:33 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-09 15:33:33 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-09 15:33:50 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 15:33:50 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 15:33:50 - __main__ - INFO - Storage directory ready: data
2026-02-09 15:33:50 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 15:33:50 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 15:33:50 - __main__ - INFO - Path validation complete
2026-02-09 15:33:50 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 15:33:50 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 15:33:50 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 15:33:50 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 15:33:51 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 15:33:51 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:33:51 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:33:52 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 15:33:53 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 15:33:54 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 15:33:54 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 15:33:54 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 15:33:55 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 1/3): Failed to create llama_context
2026-02-09 15:33:55 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 2/3)
2026-02-09 15:33:56 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 2/3): Failed to create llama_context
2026-02-09 15:33:56 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 3/3)
2026-02-09 15:33:58 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 3/3): Failed to create llama_context
2026-02-09 15:33:58 - __main__ - ERROR - Failed to initialize pipeline: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 129, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:33:58 - __main__ - ERROR - Unexpected error in main loop: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 208, in async_main
    pipeline = await initialize_pipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\main.py", line 129, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:33:58 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-09 15:33:58 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-09 15:33:58 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-09 15:35:33 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 15:35:33 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 15:35:33 - __main__ - INFO - Storage directory ready: data
2026-02-09 15:35:33 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 15:35:33 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 15:35:33 - __main__ - INFO - Path validation complete
2026-02-09 15:35:33 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 15:35:33 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 15:35:33 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 15:35:33 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 15:35:33 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 15:35:33 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:35:33 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:35:34 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 15:35:36 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 15:35:36 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 15:35:36 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 15:35:36 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 15:35:37 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 1/3): Failed to create llama_context
2026-02-09 15:35:37 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 2/3)
2026-02-09 15:35:39 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 2/3): Failed to create llama_context
2026-02-09 15:35:39 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 3/3)
2026-02-09 15:35:40 - src.llm_client - ERROR - Failed to load Main LLM model (attempt 3/3): Failed to create llama_context
2026-02-09 15:35:40 - __main__ - ERROR - Failed to initialize pipeline: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 129, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:35:40 - __main__ - ERROR - Unexpected error in main loop: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
Traceback (most recent call last):
  File "D:\Neurohacks\src\llm_client.py", line 61, in _load_model_with_retries
    self.llm = Llama(
               ^^^^^^
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\llama.py", line 395, in __init__
    internals.LlamaContext(
  File "D:\Neurohacks\venv\Lib\site-packages\llama_cpp\_internals.py", line 263, in __init__
    raise ValueError("Failed to create llama_context")
ValueError: Failed to create llama_context

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 208, in async_main
    pipeline = await initialize_pipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\main.py", line 129, in initialize_pipeline
    pipeline = CognitivePipeline(config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\pipeline.py", line 95, in __init__
    self.llm = LocalLLMClient(
               ^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\llm_client.py", line 45, in __init__
    self._load_model_with_retries()
  File "D:\Neurohacks\src\llm_client.py", line 75, in _load_model_with_retries
    raise RuntimeError(
RuntimeError: Failed to load Main LLM model after 3 attempts. Please check the model path: ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:35:40 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-09 15:35:40 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-09 15:35:40 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-09 15:36:41 - __main__ - INFO - Logging initialized at level: INFO
2026-02-09 15:36:41 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-09 15:36:41 - __main__ - INFO - Storage directory ready: data
2026-02-09 15:36:41 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-09 15:36:41 - __main__ - INFO - Storage directory ready: data\archive
2026-02-09 15:36:41 - __main__ - INFO - Path validation complete
2026-02-09 15:36:41 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 15:36:41 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-09 15:36:41 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-09 15:36:41 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-09 15:36:42 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-09 15:36:42 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:36:42 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-09 15:36:43 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-09 15:36:45 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-09 15:36:45 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-09 15:36:45 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-09 15:36:45 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-09 15:36:47 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-09 15:36:47 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-09 15:36:47 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-09 15:36:47 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-09 15:36:47 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-09 15:36:47 - src.pipeline - INFO - Starting background tasks...
2026-02-09 15:36:47 - src.pipeline - INFO - Reflection loop started
2026-02-09 15:36:47 - src.pipeline - INFO - Decay manager started
2026-02-09 15:36:47 - src.pipeline - INFO - All background tasks started successfully
2026-02-09 15:36:47 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-09 15:36:47 - __main__ - INFO - Pipeline initialized successfully
2026-02-09 15:36:47 - src.terminal_ui - INFO - New conversation started with session_id: f9687d94-9c07-4719-8f06-9574ca3960e6
2026-02-09 15:36:47 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-09 15:36:47 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-09 15:36:47 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-09 15:37:29 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Vague acknowledgment
Triples: []'
2026-02-09 15:37:29 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-09 15:37:29 - src.pipeline - INFO - Processing turn 1: Remind me to call mom...
2026-02-09 15:37:32 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Vague acknowledgment
Triples: []'
2026-02-09 15:37:32 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-09 15:37:36 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '4', using pattern fallback
2026-02-09 15:37:37 - src.retrieval_gatekeeper - INFO - Retrieval completed in 5082.18ms: 0 memories, 128 tokens
2026-02-09 15:37:45 - src.pipeline - INFO - Response generated: I can set a reminder for you. Would you like me to...
2026-02-09 15:37:45 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-09 15:38:09 - src.memory_analyzer - INFO - SLM raw output: 'Category: PINNED
Confidence: 1.0
Key info: User's name is John
Triples: [User, HAS_NAME, John]'
2026-02-09 15:38:09 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=1.00
2026-02-09 15:38:09 - src.pipeline - INFO - Processing turn 2: my name is john...
2026-02-09 15:38:25 - src.memory_analyzer - INFO - SLM raw output: 'Category: PINNED
Confidence: 1.0
Key info: User's name is John
Triples: [User, HAS_NAME, John] 
Cate'
2026-02-09 15:38:25 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-09 15:38:50 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '4', using pattern fallback
2026-02-09 15:38:51 - src.retrieval_gatekeeper - INFO - Retrieval completed in 25867.46ms: 2 memories, 187 tokens
2026-02-09 15:39:10 - src.pipeline - INFO - Response generated: Hi John, I've noted that you'd like to be reminded...
2026-02-09 15:39:10 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-09 15:40:01 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Greeting
Triples: []'
2026-02-09 15:40:01 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-09 15:40:01 - src.pipeline - INFO - Processing turn 3: I had a terrible headache this morning...
2026-02-09 15:40:04 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: 
Triples: []'
2026-02-09 15:40:04 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-09 15:40:08 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '4', using pattern fallback
2026-02-09 15:40:09 - src.retrieval_gatekeeper - INFO - Retrieval completed in 4511.20ms: 3 memories, 227 tokens
2026-02-09 15:40:14 - src.pipeline - INFO - Response generated: Sorry to hear that, John. Hopefully, you're feelin...
2026-02-09 15:40:14 - src.pipeline - INFO - Turn 3 completed successfully
2026-02-09 15:41:14 - src.memory_analyzer - INFO - SLM raw output: 'Category: PINNED
Confidence: 1.0
Key info: User's dislike for a specific food
Triples: [User, HATES,'
2026-02-09 15:41:14 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=1.00
2026-02-09 15:41:14 - src.pipeline - INFO - Processing turn 4: I hate brocolli...
2026-02-09 15:41:18 - src.memory_analyzer - INFO - SLM raw output: 'Category: PINNED
Confidence: 1.0
Key info: User's dislike for a specific food
Triples: [User, HATES,'
2026-02-09 15:41:18 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=1.00
2026-02-09 15:41:27 - src.retrieval_gatekeeper - INFO - Retrieval completed in 8185.42ms: 3 memories, 227 tokens
2026-02-09 15:41:30 - src.pipeline - INFO - Response generated: Broccoli can be a bit of a turn-off for some peopl...
2026-02-09 15:41:30 - src.pipeline - INFO - Turn 4 completed successfully
2026-02-09 15:41:30 - src.pipeline - INFO - PINNED memory stored in category 'preferences': I hate brocolli...
2026-02-09 15:44:24 - src.memory_analyzer - INFO - SLM raw output: 'Category: PINNED
Confidence: 1.0
Key info: User's name is John
Triples: [User, HAS_NAME, John]
Categ'
2026-02-09 15:44:24 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=1.00
2026-02-09 15:44:24 - src.pipeline - INFO - Processing turn 5: I have a dentist appointment next Tuesday at 10 AM...
2026-02-09 15:44:27 - src.memory_analyzer - INFO - SLM raw output: 'Category: PINNED
Confidence: 1.0
Key info: User's name is not specified
Triples: []'
2026-02-09 15:44:27 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=1.00
2026-02-09 15:44:31 - src.retrieval_gatekeeper - INFO - Retrieval completed in 3859.35ms: 0 memories, 132 tokens
2026-02-09 15:44:37 - src.pipeline - INFO - Response generated: John, I've got that reminder for you. Don't forget...
2026-02-09 15:44:37 - src.pipeline - INFO - Turn 5 completed successfully
2026-02-09 15:44:37 - src.pipeline - INFO - PINNED memory stored in category 'user_context': I have a dentist appointment next Tuesday at 10 AM...
2026-02-09 16:36:47 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-09 16:36:49 - src.decay_manager - INFO - Decay cycle complete: processed 0 memories, archived 0
2026-02-09 17:36:49 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-09 17:36:51 - src.decay_manager - INFO - Decay cycle complete: processed 0 memories, archived 0
2026-02-09 18:36:51 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-09 18:36:53 - src.decay_manager - INFO - Decay cycle complete: processed 0 memories, archived 0
2026-02-09 19:36:53 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-09 19:36:54 - src.decay_manager - INFO - Decay cycle complete: processed 0 memories, archived 0
2026-02-09 23:27:59 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-09 23:28:01 - src.decay_manager - INFO - Decay cycle complete: processed 0 memories, archived 0
2026-02-10 22:47:58 - __main__ - INFO - Logging initialized at level: INFO
2026-02-10 22:47:58 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-10 22:47:58 - __main__ - INFO - Storage directory ready: data
2026-02-10 22:47:58 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-10 22:47:58 - __main__ - INFO - Storage directory ready: data\archive
2026-02-10 22:47:58 - __main__ - INFO - Path validation complete
2026-02-10 22:47:58 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-10 22:47:58 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-10 22:47:58 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-10 22:47:58 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-10 22:47:58 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-10 22:47:58 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-10 22:47:58 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-10 22:48:14 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-10 22:48:19 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-10 22:48:20 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-10 22:48:20 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-10 22:48:20 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-10 22:48:24 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-10 22:48:24 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-10 22:48:24 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-10 22:48:24 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-10 22:48:24 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-10 22:48:24 - src.pipeline - INFO - Starting background tasks...
2026-02-10 22:48:24 - src.pipeline - INFO - Reflection loop started
2026-02-10 22:48:24 - src.pipeline - INFO - Decay manager started
2026-02-10 22:48:24 - src.pipeline - INFO - All background tasks started successfully
2026-02-10 22:48:24 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-10 22:48:24 - __main__ - INFO - Pipeline initialized successfully
2026-02-10 22:48:24 - src.terminal_ui - INFO - New conversation started with session_id: 46a14ab9-50c3-4374-878b-8f5899b25e1b
2026-02-10 22:48:24 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-10 22:48:24 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-10 22:48:24 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-10 22:49:30 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Greeting
Triples: []'
2026-02-10 22:49:30 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-10 22:49:30 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-10 22:49:35 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Greeting
Triples: []'
2026-02-10 22:49:35 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-10 22:49:53 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-10 22:49:53 - src.retrieval_gatekeeper - INFO - Retrieval completed in 18539.47ms: 0 memories, 145 tokens
2026-02-10 22:50:11 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-10 22:50:11 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-10 22:51:13 - src.memory_analyzer - INFO - SLM raw output: 'Category: PINNED
Confidence: 1.0
Key info: User's name is Steve Harrington
Triples: [User, HAS_NAME, Steve Harrington]'
2026-02-10 22:51:13 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=1.00
2026-02-10 22:51:13 - src.pipeline - INFO - Processing turn 2: My name is steve harrington...
2026-02-10 22:51:20 - src.memory_analyzer - INFO - SLM raw output: 'Category: PINNED
Confidence: 1.0
Key info: User's name is Steve Harrington
Triples: [User, HAS_NAME, Steve Harrington]'
2026-02-10 22:51:20 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=1.00
2026-02-10 22:51:27 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-10 22:51:31 - src.retrieval_gatekeeper - INFO - Retrieval completed in 10124.64ms: 2 memories, 204 tokens
2026-02-10 22:51:34 - src.pipeline - INFO - Response generated: Hello Steve. What's on your mind? I'm here to help...
2026-02-10 22:51:34 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-10 22:51:34 - src.pipeline - INFO - PINNED memory stored in category 'identity': My name is steve harrington...
2026-02-10 22:52:48 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.85
Key info: Incorrect statement about exam timing
Triples: []'
2026-02-10 22:52:48 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.85
2026-02-10 22:52:48 - src.pipeline - INFO - Processing turn 3: tomorri had an exam today...
2026-02-10 22:52:59 - src.memory_analyzer - INFO - SLM raw output: 'Category: EPISODIC
Confidence: 0.85
Key info: User had an exam today
Triples: [User, HAD_EVENT, Exam]'
2026-02-10 22:52:59 - src.memory_analyzer - INFO - SLM classification: category=episodic, confidence=0.85
2026-02-10 22:53:09 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-10 22:53:12 - src.retrieval_gatekeeper - INFO - Retrieval completed in 12684.51ms: 3 memories, 243 tokens
2026-02-10 22:53:17 - src.pipeline - INFO - Response generated: How did the exam go, Steve? Were you prepared?...
2026-02-10 22:53:17 - src.pipeline - INFO - Turn 3 completed successfully
2026-02-10 22:53:18 - src.pipeline - INFO - Memory stored successfully: id=f5b63f3b-fde8-432e-8865-bcd4414c87dc
2026-02-10 22:54:18 - src.memory_analyzer - INFO - SLM raw output: 'Category: RELATIONAL
Confidence: 0.85
Key info: Nancy is considered good
Triples: [Nancy, IS_GOOD, True]'
2026-02-10 22:54:18 - src.memory_analyzer - INFO - SLM classification: category=relational, confidence=0.85
2026-02-10 22:54:18 - src.pipeline - INFO - Processing turn 4: nancy is good...
2026-02-10 22:54:25 - src.memory_analyzer - INFO - SLM raw output: 'Category: RELATIONAL
Confidence: 0.85
Key info: Nancy is good at something
Triples: [Nancy, IS_GOOD_AT, Something]'
2026-02-10 22:54:25 - src.memory_analyzer - INFO - SLM classification: category=relational, confidence=0.85
2026-02-10 22:54:29 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-10 22:54:33 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `CONTRADICTS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=73, offset=73>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 73, 'line': 2, 'column': 73}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-10 22:54:33 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `RELATES_TO` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=85, offset=85>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 85, 'line': 2, 'column': 85}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-10 22:54:33 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `REPLACES` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=64, offset=64>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 64, 'line': 2, 'column': 64}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-10 22:54:33 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N52', status_description='warn: property key does not exist. The property `weight` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=4, column=68, offset=236>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 236, 'line': 4, 'column': 68}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-10 22:54:33 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N52', status_description='warn: property key does not exist. The property `weight` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=7, column=81, offset=434>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 434, 'line': 7, 'column': 81}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-10 22:54:33 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `STRENGTHENS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=52, offset=52>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 52, 'line': 2, 'column': 52}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-10 22:54:34 - src.retrieval_gatekeeper - INFO - Retrieval completed in 9285.82ms: 3 memories, 243 tokens
2026-02-10 22:54:39 - src.pipeline - INFO - Response generated: It sounds like Nancy is a supportive presence in y...
2026-02-10 22:54:39 - src.pipeline - INFO - Turn 4 completed successfully
2026-02-10 22:54:40 - src.pipeline - INFO - Memory stored successfully: id=1e9c79c1-bd22-4ca3-a984-75f4baa148ed
2026-02-10 23:01:04 - src.memory_analyzer - INFO - SLM raw output: 'Category: CRITICAL
Confidence: 0.85
Key info: Definite attendance required for tomorrow's meeting
Triples: [User, HAS_COMMITMENT, tomorrow's meeting]'
2026-02-10 23:01:04 - src.memory_analyzer - INFO - SLM classification: category=critical, confidence=0.85
2026-02-10 23:01:04 - src.pipeline - INFO - Processing turn 5: broo i have to definetly attend tommorrows meeting...
2026-02-10 23:01:11 - src.memory_analyzer - INFO - SLM raw output: 'Category: CRITICAL
Confidence: 0.95
Key info: Meeting to attend tomorrow
Triples: [User, HAS_MEETING, tomorrow]'
2026-02-10 23:01:11 - src.memory_analyzer - INFO - SLM classification: category=critical, confidence=0.95
2026-02-10 23:01:30 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-10 23:01:34 - src.retrieval_gatekeeper - INFO - Retrieval completed in 23113.75ms: 4 memories, 275 tokens
2026-02-10 23:01:54 - src.pipeline - INFO - Response generated: Sounds like you've got a priority to attend to tom...
2026-02-10 23:01:54 - src.pipeline - INFO - Turn 5 completed successfully
2026-02-10 23:01:56 - src.pipeline - INFO - Memory stored successfully: id=c8a2a85a-a2d6-4608-820c-b388666c3ab5
2026-02-10 23:01:57 - src.graph_engine - INFO - Merged 1 memories into 4977cac4-fa62-485b-8bab-68ccc8f503d5
2026-02-10 23:01:57 - src.reflection_loop - INFO - Reflection Loop merged 2 memories into 4977cac4-fa62-485b-8bab-68ccc8f503d5
2026-02-10 23:05:47 - src.memory_analyzer - INFO - SLM raw output: 'Category: RELATIONAL
Confidence: 0.85
Key info: User mentioned someone being good but didn't specify who
Triples: [User, REMEMBER, someone is good]'
2026-02-10 23:05:47 - src.memory_analyzer - INFO - SLM classification: category=relational, confidence=0.85
2026-02-10 23:05:47 - src.pipeline - INFO - Processing turn 6: i told you someone is good, who was it?...
2026-02-10 23:05:56 - src.memory_analyzer - INFO - SLM raw output: 'Category: RELATIONAL
Confidence: 0.85
Key info: User mentioned someone being good, but didn't specify who
Triples: [User, MENTIONED_PERSON, unknown]'
2026-02-10 23:05:56 - src.memory_analyzer - INFO - SLM classification: category=relational, confidence=0.85
2026-02-10 23:06:13 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-10 23:06:16 - src.retrieval_gatekeeper - INFO - Retrieval completed in 20218.56ms: 8 memories, 405 tokens
2026-02-10 23:06:29 - src.pipeline - INFO - Response generated: You mentioned Nancy is good....
2026-02-10 23:06:29 - src.pipeline - INFO - Turn 6 completed successfully
2026-02-10 23:06:31 - src.pipeline - INFO - Memory stored successfully: id=17a45054-c41a-4fe7-bf63-5b87094fdfd0
2026-02-10 23:06:33 - src.graph_engine - INFO - Merged 1 memories into 3b547b49-9274-47bb-8891-3572b7f7de07
2026-02-10 23:06:33 - src.reflection_loop - INFO - Reflection Loop merged 2 memories into 3b547b49-9274-47bb-8891-3572b7f7de07
2026-02-10 23:06:33 - src.graph_engine - INFO - Merged 1 memories into df6d51ae-634e-48db-bd0a-972bca4dafc8
2026-02-10 23:06:33 - src.reflection_loop - INFO - Reflection Loop merged 2 memories into df6d51ae-634e-48db-bd0a-972bca4dafc8
2026-02-10 23:10:06 - __main__ - INFO - Shutting down...
2026-02-10 23:10:06 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-10 23:10:06 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-10 23:10:06 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-10 23:10:07 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-10 23:10:07 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-10 23:10:07 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-10 23:10:08 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-11 00:14:01 - __main__ - INFO - Logging initialized at level: INFO
2026-02-11 00:14:01 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-11 00:14:01 - __main__ - INFO - Storage directory ready: data
2026-02-11 00:14:01 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-11 00:14:01 - __main__ - INFO - Storage directory ready: data\archive
2026-02-11 00:14:01 - __main__ - INFO - Path validation complete
2026-02-11 00:14:01 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-11 00:14:01 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-11 00:14:01 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-11 00:14:01 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-11 00:14:01 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-11 00:14:01 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-11 00:14:01 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-11 00:14:03 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-11 00:14:04 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-11 00:14:05 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-11 00:14:05 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-11 00:14:05 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf (attempt 1/3)
2026-02-11 00:14:06 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-11 00:14:06 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-11 00:14:06 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-11 00:14:06 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-11 00:14:06 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-11 00:14:06 - src.pipeline - INFO - Starting background tasks...
2026-02-11 00:14:06 - src.pipeline - INFO - Reflection loop started
2026-02-11 00:14:06 - src.pipeline - INFO - Decay manager started
2026-02-11 00:14:06 - src.pipeline - INFO - All background tasks started successfully
2026-02-11 00:14:06 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-11 00:14:06 - __main__ - INFO - Pipeline initialized successfully
2026-02-11 00:14:06 - src.terminal_ui - INFO - New conversation started with session_id: 39720407-0fde-4eca-9eee-fe142aa4b9fa
2026-02-11 00:14:06 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-11 00:14:06 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-11 00:14:06 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-11 00:15:23 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Greeting
Triples: []'
2026-02-11 00:15:23 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-11 00:15:23 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-11 00:15:28 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Greeting
Triples: []'
2026-02-11 00:15:28 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-11 00:15:31 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '4.', using pattern fallback
2026-02-11 00:15:31 - src.retrieval_gatekeeper - INFO - Retrieval completed in 3534.83ms: 0 memories, 152 tokens
2026-02-11 00:15:31 - src.pipeline - ERROR - Failed to generate response: 'coroutine' object is not subscriptable
Traceback (most recent call last):
  File "D:\Neurohacks\src\pipeline.py", line 229, in process_turn
    logger.info(f"Response generated: {response[:50]}...")
                                       ~~~~~~~~^^^^^
TypeError: 'coroutine' object is not subscriptable
2026-02-11 00:15:31 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-11 00:16:02 - __main__ - INFO - Shutting down...
2026-02-11 00:16:02 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-11 00:16:02 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-11 00:16:02 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-11 00:16:02 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-11 00:16:02 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-11 00:16:02 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-11 00:16:03 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-11 00:16:14 - __main__ - INFO - Logging initialized at level: INFO
2026-02-11 00:16:14 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-11 00:16:14 - __main__ - INFO - Storage directory ready: data
2026-02-11 00:16:14 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-11 00:16:14 - __main__ - INFO - Storage directory ready: data\archive
2026-02-11 00:16:14 - __main__ - INFO - Path validation complete
2026-02-11 00:16:14 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-11 00:16:14 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-11 00:16:14 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-11 00:16:14 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-11 00:16:15 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-11 00:16:15 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-11 00:16:15 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf
2026-02-11 00:16:16 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-11 00:16:19 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-11 00:16:20 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-11 00:16:20 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-11 00:16:20 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-1B-Instruct-Q6_K_L.gguf (attempt 1/3)
2026-02-11 00:16:21 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-11 00:16:21 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-11 00:16:21 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-11 00:16:21 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-11 00:16:21 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-11 00:16:21 - src.pipeline - INFO - Starting background tasks...
2026-02-11 00:16:21 - src.pipeline - INFO - Reflection loop started
2026-02-11 00:16:21 - src.pipeline - INFO - Decay manager started
2026-02-11 00:16:21 - src.pipeline - INFO - All background tasks started successfully
2026-02-11 00:16:21 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-11 00:16:21 - __main__ - INFO - Pipeline initialized successfully
2026-02-11 00:16:21 - src.terminal_ui - INFO - New conversation started with session_id: 3c2684dd-9394-4bf4-8b8e-1ff061e762a4
2026-02-11 00:16:21 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-11 00:16:21 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-11 00:16:21 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-11 00:16:37 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Greeting
Triples: []'
2026-02-11 00:16:37 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-11 00:16:37 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-11 00:16:41 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Greeting
Triples: []'
2026-02-11 00:16:41 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-11 00:16:44 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '4.', using pattern fallback
2026-02-11 00:16:44 - src.retrieval_gatekeeper - INFO - Retrieval completed in 2915.69ms: 0 memories, 152 tokens
2026-02-11 00:16:44 - src.pipeline - ERROR - Failed to generate response: 'coroutine' object is not subscriptable
Traceback (most recent call last):
  File "D:\Neurohacks\src\pipeline.py", line 229, in process_turn
    logger.info(f"Response generated: {response[:50]}...")
                                       ~~~~~~~~^^^^^
TypeError: 'coroutine' object is not subscriptable
2026-02-11 00:16:44 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-11 00:17:13 - src.memory_analyzer - INFO - SLM raw output: 'Category: PINNED
Confidence: 1.0
Key info: User's name is Clark Kent
Triples: [User, HAS_NAME, Clark Kent]'
2026-02-11 00:17:13 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=1.00
2026-02-11 00:17:13 - src.pipeline - INFO - Processing turn 2: my name is clark kent...
2026-02-11 00:17:22 - src.memory_analyzer - INFO - SLM raw output: 'Category: PINNED
Confidence: 1.0
Key info: User's name is Clark Kent
Triples: [User, HAS_NAME, Clark Kent]'
2026-02-11 00:17:22 - src.memory_analyzer - INFO - SLM classification: category=pinned, confidence=1.00
2026-02-11 00:17:25 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '4', using pattern fallback
2026-02-11 00:17:27 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `STRENGTHENS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=52, offset=52>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 52, 'line': 2, 'column': 52}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-11 00:17:27 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `REPLACES` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=64, offset=64>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 64, 'line': 2, 'column': 64}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-11 00:17:27 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `CONTRADICTS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=73, offset=73>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 73, 'line': 2, 'column': 73}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-11 00:17:27 - src.retrieval_gatekeeper - INFO - Retrieval completed in 4816.30ms: 0 memories, 152 tokens
2026-02-11 00:17:27 - src.pipeline - ERROR - Failed to generate response: 'coroutine' object is not subscriptable
Traceback (most recent call last):
  File "D:\Neurohacks\src\pipeline.py", line 229, in process_turn
    logger.info(f"Response generated: {response[:50]}...")
                                       ~~~~~~~~^^^^^
TypeError: 'coroutine' object is not subscriptable
2026-02-11 00:17:27 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-11 00:17:27 - src.pipeline - INFO - PINNED memory stored in category 'identity': my name is clark kent...
2026-02-11 00:17:51 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Greeting
Triples: []'
2026-02-11 00:17:51 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-11 00:17:51 - src.pipeline - INFO - Processing turn 3: i really have to go to meeting tomorrow...
2026-02-11 00:17:54 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Vague acknowledgment
Triples: []'
2026-02-11 00:17:54 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-11 00:17:57 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '4', using pattern fallback
2026-02-11 00:17:58 - src.retrieval_gatekeeper - INFO - Retrieval completed in 3865.46ms: 0 memories, 158 tokens
2026-02-11 00:17:58 - src.pipeline - ERROR - Failed to generate response: 'coroutine' object is not subscriptable
Traceback (most recent call last):
  File "D:\Neurohacks\src\pipeline.py", line 229, in process_turn
    logger.info(f"Response generated: {response[:50]}...")
                                       ~~~~~~~~^^^^^
TypeError: 'coroutine' object is not subscriptable
2026-02-11 00:17:58 - src.pipeline - INFO - Turn 3 completed successfully
2026-02-11 00:18:39 - src.memory_analyzer - INFO - SLM raw output: 'Category: PINNED
Confidence: 1.0
Key info: User's name is Lois
Triples: [User, HAS_NAME, Lois]
Category: RELATIONAL
Confidence: 0.8
Key info: User's news reporter is Lois
Triples: [User, IS_REPORTER_OF, Lois]'
2026-02-11 00:18:39 - src.memory_analyzer - INFO - SLM classification: category=relational, confidence=0.80
2026-02-11 00:18:39 - src.pipeline - INFO - Processing turn 4: lois is my news reporter...
2026-02-11 00:19:18 - src.memory_analyzer - INFO - SLM raw output: 'Category: PINNED
Confidence: 1.0
Key info: User's name is Lois
Triples: [User, HAS_NAME, Lois]
Category: RELATIONAL
Confidence: 0.8
Key info: User's news reporter is Lois
Triples: [User, IS_REPORTER_OF, Lois] 
Category: DISCARD
Confidence: 0.9
Key info: Greeting
Triples: [] 
Category: DISCARD
Confidence: 0.9
Key info: Vague acknowledgment
Triples: [] 
Category: DISCARD
Confidence: 0.9
Key info: Chit-chat
Triples: [] 
Category: DISCARD
Confidence: 0.9
Key info: Acknowledgment
Triples: [] 
Categor'
2026-02-11 00:19:18 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.00
2026-02-11 00:19:21 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '3', using pattern fallback
2026-02-11 00:19:22 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `STRENGTHENS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=52, offset=52>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 52, 'line': 2, 'column': 52}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-11 00:19:22 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `REPLACES` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=64, offset=64>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 64, 'line': 2, 'column': 64}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-11 00:19:22 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `CONTRADICTS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=73, offset=73>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 73, 'line': 2, 'column': 73}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-11 00:19:22 - src.retrieval_gatekeeper - INFO - Retrieval completed in 4132.93ms: 0 memories, 158 tokens
2026-02-11 00:19:22 - src.pipeline - ERROR - Failed to generate response: 'coroutine' object is not subscriptable
Traceback (most recent call last):
  File "D:\Neurohacks\src\pipeline.py", line 229, in process_turn
    logger.info(f"Response generated: {response[:50]}...")
                                       ~~~~~~~~^^^^^
TypeError: 'coroutine' object is not subscriptable
2026-02-11 00:19:22 - src.pipeline - INFO - Turn 4 completed successfully
2026-02-11 00:22:31 - src.memory_analyzer - INFO - SLM raw output: 'Category: TEMPORAL
Confidence: 0.8
Key info: Reminder to set alarm at 5pm
Triples: [User, NEEDS_ALARM, 5pm]'
2026-02-11 00:22:31 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.80
2026-02-11 00:22:31 - src.pipeline - INFO - Processing turn 5: set an alarm for 5pm...
2026-02-11 00:22:37 - src.memory_analyzer - INFO - SLM raw output: 'Category: TEMPORAL
Confidence: 0.8
Key info: Reminder to set alarm at 5pm
Triples: []'
2026-02-11 00:22:37 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.80
2026-02-11 00:22:51 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '4', using pattern fallback
2026-02-11 00:22:55 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `STRENGTHENS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=52, offset=52>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 52, 'line': 2, 'column': 52}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-11 00:22:55 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `REPLACES` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=64, offset=64>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 64, 'line': 2, 'column': 64}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-11 00:22:55 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `CONTRADICTS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=73, offset=73>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 73, 'line': 2, 'column': 73}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-11 00:22:55 - src.retrieval_gatekeeper - INFO - Retrieval completed in 17992.62ms: 0 memories, 158 tokens
2026-02-11 00:22:55 - src.pipeline - ERROR - Failed to generate response: 'coroutine' object is not subscriptable
Traceback (most recent call last):
  File "D:\Neurohacks\src\pipeline.py", line 229, in process_turn
    logger.info(f"Response generated: {response[:50]}...")
                                       ~~~~~~~~^^^^^
TypeError: 'coroutine' object is not subscriptable
2026-02-11 00:22:55 - src.pipeline - INFO - Turn 5 completed successfully
2026-02-11 00:24:46 - __main__ - INFO - Shutting down...
2026-02-11 00:24:46 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-11 00:24:46 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-11 00:24:46 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-11 00:24:46 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-11 00:24:46 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-11 00:24:46 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-11 00:24:47 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-11 00:25:12 - __main__ - INFO - Logging initialized at level: INFO
2026-02-11 00:25:12 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-11 00:25:12 - __main__ - INFO - Storage directory ready: data
2026-02-11 00:25:12 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-11 00:25:12 - __main__ - INFO - Storage directory ready: data\archive
2026-02-11 00:25:12 - __main__ - INFO - Path validation complete
2026-02-11 00:25:12 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-11 00:25:12 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-11 00:25:12 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-11 00:25:12 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-11 00:25:13 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-11 00:25:13 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-11 00:25:13 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-11 00:25:26 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-11 00:25:27 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-11 00:25:28 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-11 00:25:28 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-11 00:25:28 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-11 00:25:30 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-11 00:25:30 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-11 00:25:30 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-11 00:25:30 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-11 00:25:30 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-11 00:25:30 - src.pipeline - INFO - Starting background tasks...
2026-02-11 00:25:30 - src.pipeline - INFO - Reflection loop started
2026-02-11 00:25:30 - src.pipeline - INFO - Decay manager started
2026-02-11 00:25:30 - src.pipeline - INFO - All background tasks started successfully
2026-02-11 00:25:30 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-11 00:25:30 - __main__ - INFO - Pipeline initialized successfully
2026-02-11 00:25:30 - src.terminal_ui - INFO - New conversation started with session_id: b50d7cc1-e518-462f-9ec5-db46634d6830
2026-02-11 00:25:31 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-11 00:25:31 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-11 00:25:31 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-11 00:28:09 - src.memory_analyzer - INFO - SLM raw output: 'Category: RELATIONAL
Confidence: 0.95
Key info: Information about another person's pet
Triples: [bruno, IS_PET_OF, jojo]'
2026-02-11 00:28:09 - src.memory_analyzer - INFO - SLM classification: category=relational, confidence=0.95
2026-02-11 00:28:09 - src.pipeline - INFO - Processing turn 1: bruno is jojo's dog...
2026-02-11 00:28:24 - src.memory_analyzer - INFO - SLM raw output: 'Category: RELATIONAL
Confidence: 1.0
Key info: Information about another person's pet
Triples: [Bruno, IS_PET_OF, Jojo]'
2026-02-11 00:28:24 - src.memory_analyzer - INFO - SLM classification: category=relational, confidence=1.00
2026-02-11 00:28:36 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-11 00:28:38 - src.retrieval_gatekeeper - INFO - Retrieval completed in 13850.27ms: 2 memories, 198 tokens
2026-02-11 00:28:38 - src.pipeline - ERROR - Failed to generate response: 'coroutine' object is not subscriptable
Traceback (most recent call last):
  File "D:\Neurohacks\src\pipeline.py", line 229, in process_turn
    logger.info(f"Response generated: {response[:50]}...")
                                       ~~~~~~~~^^^^^
TypeError: 'coroutine' object is not subscriptable
2026-02-11 00:28:38 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-11 00:28:39 - src.pipeline - INFO - Memory stored successfully: id=987ddb3d-d293-4b56-b19a-e7b5da668411
2026-02-11 00:28:39 - src.graph_engine - INFO - Merged 1 memories into 2b190847-fe4d-4570-823b-f1768b677744
2026-02-11 00:28:39 - src.reflection_loop - INFO - Reflection Loop merged 2 memories into 2b190847-fe4d-4570-823b-f1768b677744
2026-02-11 00:42:57 - __main__ - INFO - Logging initialized at level: INFO
2026-02-11 00:42:57 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-11 00:42:57 - __main__ - INFO - Storage directory ready: data
2026-02-11 00:42:57 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-11 00:42:57 - __main__ - INFO - Storage directory ready: data\archive
2026-02-11 00:42:57 - __main__ - INFO - Path validation complete
2026-02-11 00:42:57 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-11 00:42:57 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-11 00:42:57 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-11 00:42:57 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-11 00:42:58 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-11 00:42:58 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-11 00:42:58 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-11 00:43:09 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-11 00:43:10 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-11 00:43:10 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-11 00:43:10 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-11 00:43:10 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-11 00:43:12 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-11 00:43:12 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-11 00:43:12 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-11 00:43:12 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-11 00:43:12 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-11 00:43:12 - src.pipeline - INFO - Starting background tasks...
2026-02-11 00:43:12 - src.pipeline - INFO - Reflection loop started
2026-02-11 00:43:12 - src.pipeline - INFO - Decay manager started
2026-02-11 00:43:12 - src.pipeline - INFO - All background tasks started successfully
2026-02-11 00:43:12 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-11 00:43:12 - __main__ - INFO - Pipeline initialized successfully
2026-02-11 00:43:12 - src.terminal_ui - INFO - New conversation started with session_id: db5af34b-6a5d-4940-85c8-3f2b901e6029
2026-02-11 00:43:12 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-11 00:43:12 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-11 00:43:12 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-11 00:44:44 - src.memory_analyzer - INFO - SLM raw output: 'Category: RELATIONAL
Confidence: 0.98
Key info: Casper has a new pet named Griffin
Entities: [griffin, casper]
Triples: [griffin, IS_PET_OF, casper]'
2026-02-11 00:44:44 - src.memory_analyzer - INFO - SLM classification: category=relational, confidence=0.98
2026-02-11 00:44:44 - src.pipeline - INFO - Processing turn 1: griffin is casper's new pet...
2026-02-11 00:45:03 - src.memory_analyzer - INFO - SLM raw output: 'Category: RELATIONAL
Confidence: 0.95
Key info: Casper has a new pet named Griffin
Entities: [griffin, casper]
Triples: [griffin, IS_PET_OF, casper]'
2026-02-11 00:45:03 - src.memory_analyzer - INFO - SLM classification: category=relational, confidence=0.95
2026-02-11 00:45:19 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-11 00:45:21 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `STRENGTHENS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=52, offset=52>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 52, 'line': 2, 'column': 52}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-11 00:45:21 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `REPLACES` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=64, offset=64>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 64, 'line': 2, 'column': 64}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-11 00:45:21 - neo4j.notifications - WARNING - Received notification from DBMS server: <GqlStatusObject gql_status='01N51', status_description='warn: relationship type does not exist. The relationship type `CONTRADICTS` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=2, column=73, offset=73>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 73, 'line': 2, 'column': 73}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: '\n                    MATCH path = (start:Memory)-[r:STRENGTHENS|REPLACES|CONTRADICTS|RELATES_TO*1..2]->(related:Memory)\n                    WHERE start.id IN $start_ids\n                      AND ALL(rel IN relationships(path) WHERE rel.weight > $min_weight)\n                      AND related.confidence > 0.1\n                    RETURN DISTINCT related,\n                           reduce(w = 1.0, rel IN relationships(path) | w * rel.weight) AS path_weight\n                    ORDER BY path_weight DESC\n                    LIMIT 50\n                '
2026-02-11 00:45:21 - src.retrieval_gatekeeper - INFO - Retrieval completed in 18312.60ms: 0 memories, 158 tokens
2026-02-11 00:45:21 - src.pipeline - ERROR - Failed to generate response: 'coroutine' object is not subscriptable
Traceback (most recent call last):
  File "D:\Neurohacks\src\pipeline.py", line 229, in process_turn
    logger.info(f"Response generated: {response[:50]}...")
                                       ~~~~~~~~^^^^^
TypeError: 'coroutine' object is not subscriptable
2026-02-11 00:45:22 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-11 00:45:24 - src.pipeline - INFO - Memory stored successfully: id=27d9e404-affd-4dd0-9904-931e0593df44
2026-02-11 00:49:25 - __main__ - INFO - Shutting down...
2026-02-11 00:49:25 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-11 00:49:25 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-11 00:49:25 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-11 00:49:25 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-11 00:49:25 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-11 00:49:25 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-11 00:49:26 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-11 00:49:31 - __main__ - INFO - Logging initialized at level: INFO
2026-02-11 00:49:31 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-11 00:49:31 - __main__ - INFO - Storage directory ready: data
2026-02-11 00:49:31 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-11 00:49:31 - __main__ - INFO - Storage directory ready: data\archive
2026-02-11 00:49:31 - __main__ - INFO - Path validation complete
2026-02-11 00:49:31 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-11 00:49:31 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-11 00:49:31 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-11 00:49:31 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-11 00:49:31 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-11 00:49:31 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-11 00:49:31 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-11 00:49:46 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-11 00:49:49 - src.graph_engine - INFO - Successfully connected to Neo4j database
2026-02-11 00:49:49 - src.graph_engine - INFO - Created vector index for embeddings
2026-02-11 00:49:49 - src.graph_engine - INFO - Successfully created Neo4j schema
2026-02-11 00:49:49 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-11 00:49:50 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-11 00:49:50 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-11 00:49:50 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-11 00:49:50 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-11 00:49:50 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-11 00:49:50 - src.pipeline - INFO - Starting background tasks...
2026-02-11 00:49:50 - src.pipeline - INFO - Reflection loop started
2026-02-11 00:49:50 - src.pipeline - INFO - Decay manager started
2026-02-11 00:49:50 - src.pipeline - INFO - All background tasks started successfully
2026-02-11 00:49:50 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-11 00:49:50 - __main__ - INFO - Pipeline initialized successfully
2026-02-11 00:49:50 - src.terminal_ui - INFO - New conversation started with session_id: da7104e3-d2db-4a6d-9f58-eea906031f72
2026-02-11 00:49:50 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-11 00:49:50 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-11 00:49:50 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-11 00:50:19 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Greeting
Triples: []'
2026-02-11 00:50:19 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-11 00:50:19 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-11 00:50:25 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Greeting
Triples: []'
2026-02-11 00:50:25 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-11 00:50:29 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-11 00:50:29 - src.retrieval_gatekeeper - INFO - Retrieval completed in 3827.43ms: 0 memories, 158 tokens
2026-02-11 00:50:35 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-11 00:50:35 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-11 00:53:19 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Greeting
Triples: []'
2026-02-11 00:53:19 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-11 00:53:19 - src.pipeline - INFO - Processing turn 2: yo...
2026-02-11 00:53:24 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Greeting
Triples: []'
2026-02-11 00:53:24 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-11 00:53:28 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-11 00:53:30 - src.retrieval_gatekeeper - INFO - Retrieval completed in 5302.36ms: 7 memories, 339 tokens
2026-02-11 00:53:34 - src.pipeline - INFO - Response generated: Is everything okay?...
2026-02-11 00:53:34 - src.pipeline - INFO - Turn 2 completed successfully
2026-02-11 00:53:36 - src.graph_engine - INFO - Merged 1 memories into 2e6585f3-87c2-4901-88f8-bf086e20f8c1
2026-02-11 00:53:36 - src.reflection_loop - INFO - Reflection Loop merged 2 memories into 2e6585f3-87c2-4901-88f8-bf086e20f8c1
2026-02-11 00:55:55 - __main__ - INFO - Shutting down...
2026-02-11 00:55:55 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-11 00:55:55 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-11 00:55:55 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-11 00:55:56 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-11 00:55:56 - src.graph_engine - INFO - Closed Neo4j connection
2026-02-11 00:55:56 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-11 00:55:57 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-11 01:25:31 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-11 01:25:34 - src.decay_manager - INFO - Decay cycle complete: processed 8 memories, archived 0
2026-02-11 02:25:34 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-11 02:25:37 - src.decay_manager - INFO - Decay cycle complete: processed 11 memories, archived 0
2026-02-11 03:25:37 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-11 03:25:40 - src.decay_manager - INFO - Decay cycle complete: processed 11 memories, archived 0
2026-02-11 10:52:27 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-11 10:52:30 - src.decay_manager - INFO - Decay cycle complete: processed 11 memories, archived 0
2026-02-11 11:52:30 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-11 11:52:34 - src.decay_manager - INFO - Decay cycle complete: processed 17 memories, archived 0
2026-02-11 12:52:34 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-11 12:52:38 - src.decay_manager - INFO - Decay cycle complete: processed 17 memories, archived 0
2026-02-11 13:52:38 - src.decay_manager - INFO - Applying decay policies to all active memories
2026-02-11 13:52:42 - src.decay_manager - INFO - Decay cycle complete: processed 17 memories, archived 0
2026-02-12 08:28:57 - __main__ - INFO - Logging initialized at level: INFO
2026-02-12 08:28:57 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-12 08:28:57 - __main__ - INFO - Storage directory ready: data
2026-02-12 08:28:57 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-12 08:28:57 - __main__ - INFO - Storage directory ready: data\archive
2026-02-12 08:28:57 - __main__ - INFO - Path validation complete
2026-02-12 08:28:57 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:28:57 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:28:57 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-12 08:28:57 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-12 08:28:57 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-12 08:28:57 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:28:57 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:29:07 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-12 08:29:07 - src.graph_engine - INFO - Loaded graph with 150 nodes and 0 edges
2026-02-12 08:29:07 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-12 08:29:08 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-12 08:29:08 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-12 08:29:08 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-12 08:29:08 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-12 08:29:08 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:29:08 - src.pipeline - INFO - Starting background tasks...
2026-02-12 08:29:08 - src.pipeline - INFO - Reflection loop started
2026-02-12 08:29:08 - src.pipeline - INFO - Decay manager started
2026-02-12 08:29:08 - src.pipeline - INFO - All background tasks started successfully
2026-02-12 08:29:08 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:29:08 - __main__ - INFO - Pipeline initialized successfully
2026-02-12 08:29:08 - src.terminal_ui - INFO - New conversation started with session_id: 7df0703a-e2b7-4503-8ecc-7e0dcb691022
2026-02-12 08:29:08 - __main__ - ERROR - Unexpected error in main loop: name 'Path' is not defined
Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 223, in async_main
    await run_terminal_interface(pipeline)
  File "D:\Neurohacks\src\terminal_ui.py", line 192, in run_terminal_interface
    ui = TerminalUI(pipeline)
         ^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\terminal_ui.py", line 43, in __init__
    self._setup_session_logger()
  File "D:\Neurohacks\src\terminal_ui.py", line 50, in _setup_session_logger
    log_dir = Path("logs")
              ^^^^
NameError: name 'Path' is not defined
2026-02-12 08:29:08 - src.pipeline - INFO - Stopping background tasks...
2026-02-12 08:29:08 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-12 08:29:08 - src.decay_manager - INFO - Decay Manager stopped
2026-02-12 08:29:08 - src.pipeline - INFO - All background tasks stopped
2026-02-12 08:29:08 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-12 08:29:09 - src.graph_engine - INFO - Graph Memory Engine closed
2026-02-12 08:29:09 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-12 08:29:10 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-12 08:29:50 - __main__ - INFO - Logging initialized at level: INFO
2026-02-12 08:29:50 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-12 08:29:50 - __main__ - INFO - Storage directory ready: data
2026-02-12 08:29:50 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-12 08:29:50 - __main__ - INFO - Storage directory ready: data\archive
2026-02-12 08:29:50 - __main__ - INFO - Path validation complete
2026-02-12 08:29:50 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:29:50 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:29:50 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-12 08:29:50 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-12 08:29:50 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-12 08:29:50 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:29:50 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:29:58 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-12 08:29:58 - src.graph_engine - INFO - Loaded graph with 150 nodes and 0 edges
2026-02-12 08:29:58 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-12 08:29:59 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-12 08:29:59 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-12 08:29:59 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-12 08:29:59 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-12 08:29:59 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:29:59 - src.pipeline - INFO - Starting background tasks...
2026-02-12 08:29:59 - src.pipeline - INFO - Reflection loop started
2026-02-12 08:29:59 - src.pipeline - INFO - Decay manager started
2026-02-12 08:29:59 - src.pipeline - INFO - All background tasks started successfully
2026-02-12 08:29:59 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:29:59 - __main__ - INFO - Pipeline initialized successfully
2026-02-12 08:29:59 - src.terminal_ui - INFO - New conversation started with session_id: a5da8a4c-c1db-49bd-ba05-ea5a6693e16b
2026-02-12 08:29:59 - __main__ - ERROR - Unexpected error in main loop: name 'Path' is not defined
Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 223, in async_main
    await run_terminal_interface(pipeline)
  File "D:\Neurohacks\src\terminal_ui.py", line 192, in run_terminal_interface
    ui = TerminalUI(pipeline)
         ^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\terminal_ui.py", line 43, in __init__
    self._setup_session_logger()
  File "D:\Neurohacks\src\terminal_ui.py", line 50, in _setup_session_logger
    log_dir = Path("logs")
              ^^^^
NameError: name 'Path' is not defined
2026-02-12 08:29:59 - src.pipeline - INFO - Stopping background tasks...
2026-02-12 08:29:59 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-12 08:29:59 - src.decay_manager - INFO - Decay Manager stopped
2026-02-12 08:29:59 - src.pipeline - INFO - All background tasks stopped
2026-02-12 08:29:59 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-12 08:29:59 - src.graph_engine - INFO - Graph Memory Engine closed
2026-02-12 08:29:59 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-12 08:30:00 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-12 08:30:51 - __main__ - INFO - Logging initialized at level: INFO
2026-02-12 08:30:51 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-12 08:30:51 - __main__ - INFO - Storage directory ready: data
2026-02-12 08:30:51 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-12 08:30:51 - __main__ - INFO - Storage directory ready: data\archive
2026-02-12 08:30:51 - __main__ - INFO - Path validation complete
2026-02-12 08:30:51 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:30:51 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:30:51 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-12 08:30:51 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-12 08:30:51 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-12 08:30:51 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:30:51 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:30:59 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-12 08:30:59 - src.graph_engine - INFO - Loaded graph with 150 nodes and 0 edges
2026-02-12 08:30:59 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-12 08:31:01 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-12 08:31:01 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-12 08:31:01 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-12 08:31:01 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-12 08:31:01 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:31:01 - src.pipeline - INFO - Starting background tasks...
2026-02-12 08:31:01 - src.pipeline - INFO - Reflection loop started
2026-02-12 08:31:01 - src.pipeline - INFO - Decay manager started
2026-02-12 08:31:01 - src.pipeline - INFO - All background tasks started successfully
2026-02-12 08:31:01 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:31:01 - __main__ - INFO - Pipeline initialized successfully
2026-02-12 08:31:01 - src.terminal_ui - INFO - New conversation started with session_id: 64cdbbbb-3622-4727-8a47-3bca7044aae6
2026-02-12 08:31:01 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-12 08:31:01 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-12 08:31:01 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-12 08:31:25 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Greeting
Triples: []'
2026-02-12 08:31:25 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-12 08:31:25 - src.pipeline - INFO - Processing turn 1: Hi...
2026-02-12 08:31:29 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Greeting
Triples: []'
2026-02-12 08:31:29 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-12 08:31:32 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-12 08:31:32 - src.retrieval_gatekeeper - INFO - Retrieval completed in 3222.35ms: 0 memories, 158 tokens
2026-02-12 08:31:37 - src.pipeline - INFO - Response generated: How can I assist you today?...
2026-02-12 08:31:37 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-12 08:31:37 - src.reflection_loop - ERROR - Failed to increase confidence: 'GraphMemoryEngine' object has no attribute 'driver'
2026-02-12 08:31:37 - src.reflection_loop - INFO - Checking for consolidation candidates among 1 active entities
2026-02-12 08:33:00 - __main__ - INFO - Shutting down...
2026-02-12 08:33:00 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-12 08:33:00 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-12 08:33:00 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-12 08:33:06 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-12 08:33:07 - src.graph_engine - INFO - Graph Memory Engine closed
2026-02-12 08:33:07 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-12 08:33:07 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-12 08:43:04 - __main__ - INFO - Logging initialized at level: INFO
2026-02-12 08:43:04 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-12 08:43:04 - __main__ - INFO - Storage directory ready: data
2026-02-12 08:43:04 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-12 08:43:04 - __main__ - INFO - Storage directory ready: data\archive
2026-02-12 08:43:04 - __main__ - INFO - Path validation complete
2026-02-12 08:43:04 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:43:04 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:43:04 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-12 08:43:04 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-12 08:43:05 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-12 08:43:05 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:43:05 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:43:18 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-12 08:43:18 - src.graph_engine - INFO - Loaded graph with 150 nodes and 0 edges
2026-02-12 08:43:18 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-12 08:43:19 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-12 08:43:19 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-12 08:43:19 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-12 08:43:19 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-12 08:43:19 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:43:19 - src.pipeline - INFO - Starting background tasks...
2026-02-12 08:43:19 - src.pipeline - INFO - Reflection loop started
2026-02-12 08:43:19 - src.pipeline - INFO - Decay manager started
2026-02-12 08:43:19 - src.pipeline - INFO - All background tasks started successfully
2026-02-12 08:43:19 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:43:19 - __main__ - INFO - Pipeline initialized successfully
2026-02-12 08:43:19 - src.terminal_ui - INFO - New conversation started with session_id: 1f943942-ee11-4234-b6bd-472c72e77637
2026-02-12 08:43:19 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-12 08:43:19 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-12 08:43:19 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-12 08:44:01 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Greeting
Triples: []'
2026-02-12 08:44:01 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-12 08:44:01 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-12 08:44:28 - __main__ - INFO - Logging initialized at level: INFO
2026-02-12 08:44:28 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-12 08:44:28 - __main__ - INFO - Storage directory ready: data
2026-02-12 08:44:28 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-12 08:44:28 - __main__ - INFO - Storage directory ready: data\archive
2026-02-12 08:44:28 - __main__ - INFO - Path validation complete
2026-02-12 08:44:28 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:44:28 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:44:28 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-12 08:44:29 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-12 08:44:29 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-12 08:44:29 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:44:29 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:44:41 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-12 08:44:42 - src.graph_engine - INFO - Loaded graph with 150 nodes and 0 edges
2026-02-12 08:44:42 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-12 08:44:45 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-12 08:44:45 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-12 08:44:45 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-12 08:44:45 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-12 08:44:45 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:44:45 - src.pipeline - INFO - Starting background tasks...
2026-02-12 08:44:45 - src.pipeline - INFO - Reflection loop started
2026-02-12 08:44:45 - src.pipeline - INFO - Decay manager started
2026-02-12 08:44:45 - src.pipeline - INFO - All background tasks started successfully
2026-02-12 08:44:45 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:44:45 - __main__ - INFO - Pipeline initialized successfully
2026-02-12 08:44:45 - src.terminal_ui - INFO - New conversation started with session_id: 78fbda63-a31b-4b40-b418-f931d770be40
2026-02-12 08:44:45 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-12 08:44:45 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-12 08:44:45 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-12 08:45:16 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Greeting
Triples: []'
2026-02-12 08:45:16 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-12 08:45:16 - src.pipeline - INFO - Processing turn 1: hello...
2026-02-12 08:46:03 - __main__ - INFO - Logging initialized at level: INFO
2026-02-12 08:46:03 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-12 08:46:03 - __main__ - INFO - Storage directory ready: data
2026-02-12 08:46:03 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-12 08:46:03 - __main__ - INFO - Storage directory ready: data\archive
2026-02-12 08:46:03 - __main__ - INFO - Path validation complete
2026-02-12 08:46:03 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:46:03 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:46:03 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-12 08:46:03 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-12 08:46:03 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-12 08:46:03 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:46:03 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:46:17 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-12 08:46:17 - src.graph_engine - INFO - Loaded graph with 150 nodes and 0 edges
2026-02-12 08:46:17 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-12 08:46:19 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-12 08:46:19 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-12 08:46:19 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-12 08:46:19 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-12 08:46:19 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:46:19 - src.pipeline - INFO - Starting background tasks...
2026-02-12 08:46:19 - src.pipeline - INFO - Reflection loop started
2026-02-12 08:46:19 - src.pipeline - INFO - Decay manager started
2026-02-12 08:46:19 - src.pipeline - INFO - All background tasks started successfully
2026-02-12 08:46:19 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:46:19 - __main__ - INFO - Pipeline initialized successfully
2026-02-12 08:46:19 - src.terminal_ui - INFO - New conversation started with session_id: d8ecdd67-05ea-41bc-88e4-32cba73227b7
2026-02-12 08:46:19 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-12 08:46:19 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-12 08:46:19 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-12 08:46:25 - __main__ - INFO - Shutting down...
2026-02-12 08:46:25 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-12 08:46:25 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-12 08:46:25 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-12 08:46:25 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-12 08:46:25 - src.graph_engine - INFO - Graph Memory Engine closed
2026-02-12 08:46:25 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-12 08:46:26 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-12 08:46:38 - __main__ - INFO - Logging initialized at level: INFO
2026-02-12 08:46:38 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-12 08:46:38 - __main__ - INFO - Storage directory ready: data
2026-02-12 08:46:38 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-12 08:46:38 - __main__ - INFO - Storage directory ready: data\archive
2026-02-12 08:46:38 - __main__ - INFO - Path validation complete
2026-02-12 08:46:38 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:46:38 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:46:38 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-12 08:46:38 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-12 08:46:38 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-12 08:46:38 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:46:38 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:46:50 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-12 08:46:50 - src.graph_engine - INFO - Loaded graph with 150 nodes and 0 edges
2026-02-12 08:46:50 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-12 08:46:51 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-12 08:46:51 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-12 08:46:51 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-12 08:46:51 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-12 08:46:51 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:46:51 - src.pipeline - INFO - Starting background tasks...
2026-02-12 08:46:51 - src.pipeline - INFO - Reflection loop started
2026-02-12 08:46:51 - src.pipeline - INFO - Decay manager started
2026-02-12 08:46:51 - src.pipeline - INFO - All background tasks started successfully
2026-02-12 08:46:51 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:46:51 - __main__ - INFO - Pipeline initialized successfully
2026-02-12 08:46:51 - src.terminal_ui - INFO - New conversation started with session_id: 2c0178d8-5ec2-48cd-ab6f-2c7f2b0521e2
2026-02-12 08:46:51 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-12 08:46:51 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-12 08:46:51 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-12 08:47:13 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Greeting
Triples: []'
2026-02-12 08:47:13 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-12 08:47:13 - src.pipeline - INFO - Processing turn 1: hello...
2026-02-12 08:47:17 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Greeting
Triples: []'
2026-02-12 08:47:17 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-12 08:47:21 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-12 08:47:21 - src.retrieval_gatekeeper - INFO - Retrieval completed in 4219.23ms: 0 memories, 158 tokens
2026-02-12 08:47:28 - src.pipeline - INFO - Response generated: How can I assist you today?...
2026-02-12 08:47:28 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-12 08:47:28 - src.reflection_loop - ERROR - Failed to increase confidence: 'GraphMemoryEngine' object has no attribute 'driver'
2026-02-12 08:48:35 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-12 08:48:35 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-12 08:48:35 - src.pipeline - INFO - Stopping background tasks...
2026-02-12 08:48:35 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-12 08:48:35 - src.decay_manager - INFO - Decay Manager stopped
2026-02-12 08:48:35 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-12 08:48:35 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-12 08:48:35 - src.pipeline - INFO - All background tasks stopped
2026-02-12 08:48:36 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-12 08:48:36 - src.graph_engine - INFO - Graph Memory Engine closed
2026-02-12 08:48:36 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-12 08:48:36 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-12 08:59:31 - __main__ - INFO - Logging initialized at level: INFO
2026-02-12 08:59:31 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-12 08:59:31 - __main__ - INFO - Storage directory ready: data
2026-02-12 08:59:31 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-12 08:59:31 - __main__ - INFO - Storage directory ready: data\archive
2026-02-12 08:59:31 - __main__ - INFO - Path validation complete
2026-02-12 08:59:31 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:59:31 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 08:59:31 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-12 08:59:31 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-12 08:59:32 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-12 08:59:32 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:59:32 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 08:59:41 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-12 08:59:41 - src.graph_engine - INFO - Loaded graph with 150 nodes and 0 edges
2026-02-12 08:59:41 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-12 08:59:43 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-12 08:59:43 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-12 08:59:43 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-12 08:59:43 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-12 08:59:43 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:59:43 - src.pipeline - INFO - Starting background tasks...
2026-02-12 08:59:43 - src.pipeline - INFO - Reflection loop started
2026-02-12 08:59:43 - src.pipeline - INFO - Decay manager started
2026-02-12 08:59:43 - src.pipeline - INFO - All background tasks started successfully
2026-02-12 08:59:43 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-12 08:59:43 - __main__ - INFO - Pipeline initialized successfully
2026-02-12 08:59:43 - __main__ - ERROR - Unexpected error in main loop: 'TerminalUI' object has no attribute '_reset_conversation'
Traceback (most recent call last):
  File "D:\Neurohacks\src\main.py", line 223, in async_main
    await run_terminal_interface(pipeline)
  File "D:\Neurohacks\src\terminal_ui.py", line 185, in run_terminal_interface
    ui = TerminalUI(pipeline)
         ^^^^^^^^^^^^^^^^^^^^
  File "D:\Neurohacks\src\terminal_ui.py", line 41, in __init__
    self._reset_conversation()
    ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'TerminalUI' object has no attribute '_reset_conversation'
2026-02-12 08:59:43 - src.pipeline - INFO - Stopping background tasks...
2026-02-12 08:59:43 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-12 08:59:43 - src.decay_manager - INFO - Decay Manager stopped
2026-02-12 08:59:43 - src.pipeline - INFO - All background tasks stopped
2026-02-12 08:59:43 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-12 08:59:43 - src.graph_engine - INFO - Graph Memory Engine closed
2026-02-12 08:59:43 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-12 08:59:44 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-12 09:01:16 - __main__ - INFO - Logging initialized at level: INFO
2026-02-12 09:01:16 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-12 09:01:16 - __main__ - INFO - Storage directory ready: data
2026-02-12 09:01:16 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-12 09:01:16 - __main__ - INFO - Storage directory ready: data\archive
2026-02-12 09:01:16 - __main__ - INFO - Path validation complete
2026-02-12 09:01:16 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 09:01:16 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 09:01:16 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-12 09:01:16 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-12 09:01:16 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-12 09:01:16 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 09:01:16 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 09:01:28 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-12 09:01:28 - src.graph_engine - INFO - Loaded graph with 150 nodes and 0 edges
2026-02-12 09:01:28 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-12 09:01:29 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-12 09:01:29 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-12 09:01:29 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-12 09:01:29 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-12 09:01:29 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-12 09:01:29 - src.pipeline - INFO - Starting background tasks...
2026-02-12 09:01:29 - src.pipeline - INFO - Reflection loop started
2026-02-12 09:01:29 - src.pipeline - INFO - Decay manager started
2026-02-12 09:01:29 - src.pipeline - INFO - All background tasks started successfully
2026-02-12 09:01:29 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-12 09:01:29 - __main__ - INFO - Pipeline initialized successfully
2026-02-12 09:01:29 - src.terminal_ui - INFO - New conversation started with session_id: 8ad1754a-6a42-496d-8cf8-aae50831051b
2026-02-12 09:01:29 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-12 09:01:29 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-12 09:01:29 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-12 09:01:41 - src.pipeline - INFO - Processing turn 1: Hello...
2026-02-12 09:01:59 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.95
Key info: Greeting
Triples: []'
2026-02-12 09:01:59 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.95
2026-02-12 09:02:02 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-12 09:02:02 - src.retrieval_gatekeeper - INFO - Retrieval completed in 3862.62ms: 0 memories, 158 tokens
2026-02-12 09:02:08 - src.pipeline - INFO - Response generated: How can I assist you today?...
2026-02-12 09:02:08 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-12 09:02:08 - src.reflection_loop - ERROR - Failed to increase confidence: 'GraphMemoryEngine' object has no attribute 'driver'
2026-02-12 09:02:08 - src.reflection_loop - INFO - Checking for consolidation candidates among 1 active entities
2026-02-12 09:02:44 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-12 09:02:44 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-12 09:02:44 - src.pipeline - INFO - Stopping background tasks...
2026-02-12 09:02:44 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-12 09:02:44 - src.decay_manager - INFO - Decay Manager stopped
2026-02-12 09:02:44 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-12 09:02:44 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-12 09:02:44 - src.pipeline - INFO - All background tasks stopped
2026-02-12 09:02:44 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-12 09:02:44 - src.graph_engine - INFO - Graph Memory Engine closed
2026-02-12 09:02:44 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-12 09:02:45 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-12 11:17:11 - __main__ - INFO - Logging initialized at level: INFO
2026-02-12 11:17:11 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-12 11:17:11 - __main__ - INFO - Storage directory ready: data
2026-02-12 11:17:11 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-12 11:17:11 - __main__ - INFO - Storage directory ready: data\archive
2026-02-12 11:17:11 - __main__ - INFO - Path validation complete
2026-02-12 11:17:11 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 11:17:11 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 11:17:11 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-12 11:17:11 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-12 11:17:12 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-12 11:17:12 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 11:17:12 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 11:17:24 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-12 11:17:24 - src.graph_engine - INFO - Loaded graph with 150 nodes and 0 edges
2026-02-12 11:17:24 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-12 11:17:26 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-12 11:17:26 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-12 11:17:26 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-12 11:17:26 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-12 11:17:26 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-12 11:17:26 - src.pipeline - INFO - Starting background tasks...
2026-02-12 11:17:26 - src.pipeline - INFO - Reflection loop started
2026-02-12 11:17:26 - src.pipeline - INFO - Decay manager started
2026-02-12 11:17:26 - src.pipeline - INFO - All background tasks started successfully
2026-02-12 11:17:26 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-12 11:17:26 - __main__ - INFO - Pipeline initialized successfully
2026-02-12 11:17:26 - src.terminal_ui - INFO - New conversation started with session_id: 1e9568e1-945b-4e94-a1a2-f881d54878ff
2026-02-12 11:17:26 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-12 11:17:26 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-12 11:17:26 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-12 11:17:37 - src.graph_engine - INFO - Graph visualization saved to data\memory_graph_main.html
2026-02-12 11:23:20 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-12 11:23:20 - __main__ - INFO - Terminal interface closed, cleaning up...
2026-02-12 11:23:20 - src.pipeline - INFO - Stopping background tasks...
2026-02-12 11:23:20 - src.reflection_loop - INFO - ReflectionLoop stop requested
2026-02-12 11:23:20 - src.decay_manager - INFO - Decay Manager stopped
2026-02-12 11:23:20 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-12 11:23:20 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-12 11:23:20 - src.pipeline - INFO - All background tasks stopped
2026-02-12 11:23:20 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-12 11:23:20 - src.graph_engine - ERROR - Failed to save graph: 'NoneType' object is not callable
2026-02-12 11:23:20 - src.graph_engine - INFO - Graph Memory Engine closed
2026-02-12 11:23:20 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-12 11:23:21 - src.llm_client - INFO - Cleaning up Main LLM model
2026-02-12 17:34:03 - __main__ - INFO - Logging initialized at level: INFO
2026-02-12 17:34:03 - __main__ - INFO - === Cognitive Memory Controller Starting ===
2026-02-12 17:34:03 - __main__ - INFO - Storage directory ready: data
2026-02-12 17:34:03 - __main__ - INFO - Storage directory ready: data\sessions
2026-02-12 17:34:03 - __main__ - INFO - Storage directory ready: data\archive
2026-02-12 17:34:03 - __main__ - INFO - Path validation complete
2026-02-12 17:34:03 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 17:34:03 - __main__ - INFO - Initializing Cognitive Pipeline...
2026-02-12 17:34:03 - src.pipeline - INFO - Initializing Cognitive Pipeline components...
2026-02-12 17:34:03 - src.embedding_service - INFO - Initializing embedding service with model: ./data/models/mxbai-embed-large-v1_fp32.gguf
2026-02-12 17:34:04 - src.embedding_service - INFO - Embedding model loaded successfully
2026-02-12 17:34:04 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 17:34:04 - src.memory_analyzer - INFO - Loading SLM model from ./data/models/Dolphin3.0-Llama3.1-8B-Q4_K_S.gguf
2026-02-12 17:34:26 - src.memory_analyzer - INFO - SLM model loaded successfully
2026-02-12 17:34:26 - src.graph_engine - INFO - Loaded graph with 135 nodes and 0 edges
2026-02-12 17:34:26 - src.llm_client - INFO - Loading Main LLM model from ./data/models/Llama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf (attempt 1/3)
2026-02-12 17:34:28 - src.llm_client - INFO - Main LLM model loaded successfully
2026-02-12 17:34:28 - src.reflection_loop - INFO - ReflectionLoop initialized
2026-02-12 17:34:28 - src.decay_manager - INFO - Decay Manager initialized with category-specific policies
2026-02-12 17:34:28 - src.persistence - INFO - PersistenceManager initialized: data\sessions
2026-02-12 17:34:28 - src.pipeline - INFO - Cognitive Pipeline initialized successfully
2026-02-12 17:34:28 - src.pipeline - INFO - Starting background tasks...
2026-02-12 17:34:28 - src.pipeline - INFO - Reflection loop started
2026-02-12 17:34:28 - src.pipeline - INFO - Decay manager started
2026-02-12 17:34:28 - src.pipeline - INFO - All background tasks started successfully
2026-02-12 17:34:28 - __main__ - INFO - Cognitive Pipeline initialized successfully
2026-02-12 17:34:28 - __main__ - INFO - Pipeline initialized successfully
2026-02-12 17:34:28 - src.terminal_ui - INFO - New conversation started with session_id: a513a1a5-258a-4d35-b21e-ec9452630fa2
2026-02-12 17:34:28 - src.terminal_ui - INFO - TerminalUI initialized
2026-02-12 17:34:28 - src.reflection_loop - INFO - ReflectionLoop started
2026-02-12 17:34:28 - src.decay_manager - INFO - Starting Decay Manager background task (runs every hour)
2026-02-12 17:34:35 - src.pipeline - INFO - Processing turn 1: hi...
2026-02-12 17:34:54 - src.memory_analyzer - INFO - SLM raw output: 'Category: DISCARD
Confidence: 0.90
Key info: Greeting
Triples: []'
2026-02-12 17:34:54 - src.memory_analyzer - INFO - SLM classification: category=discard, confidence=0.90
2026-02-12 17:34:59 - src.retrieval_gatekeeper - WARNING - SLM gave unclear intent '', using pattern fallback
2026-02-12 17:34:59 - src.retrieval_gatekeeper - INFO - Retrieval completed in 4295.27ms: 0 memories, 174 tokens
2026-02-12 17:35:07 - src.pipeline - INFO - Response generated: Hello. How can I assist you today?...
2026-02-12 17:35:07 - src.pipeline - INFO - Turn 1 completed successfully
2026-02-12 17:39:41 - __main__ - INFO - Shutting down...
2026-02-12 17:39:41 - src.reflection_loop - INFO - ReflectionLoop cancelled
2026-02-12 17:39:41 - src.terminal_ui - INFO - Terminal UI stopped
2026-02-12 17:39:41 - src.decay_manager - INFO - Decay Manager task cancelled
2026-02-12 17:39:41 - src.pipeline - INFO - Cleaning up Cognitive Pipeline resources
2026-02-12 17:39:42 - src.graph_engine - INFO - Graph Memory Engine closed
2026-02-12 17:39:42 - src.embedding_service - INFO - Cleaning up embedding model
2026-02-12 17:39:42 - src.llm_client - INFO - Cleaning up Main LLM model
